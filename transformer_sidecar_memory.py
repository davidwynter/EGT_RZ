# -*- coding: utf-8 -*-
"""Transformer Sidecar: Bolt On Persistent State Space Memory

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pN-WqtdeZsI79C1nTZg2JKgDwZ3KYlI_

# Transformer Sidecar Memory — Affect-Gated Fast Weights (v0.3.3)

**What it is.** A drop-in, storage-efficient memory sidecar for any Hugging Face causal LM. It adds human-like, *selective* long-horizon recall without fine-tuning base weights or growing prompts.

**Core idea.** We keep a constant-size, low-rank **fast-weight matrix** and only write when a **surprise gate** fires. Surprise blends:

* **S:** probabilistic surprisal (NLL),
* **N:** novelty vs. current memory,
* **A:** **affect/arousal** (lexicon + numeric outliers + named-entity priors),
* **C:** conflict with known facts (optional boost),
* **R:** reward (hook ready).

Writes store the **prediction-error residual** (what’s new), with **eligibility traces** to bind nearby events. Reads use a soft Hopfield-style lookup. Deterministic fact recall is supported via **KV-injection** with a lexicon mask.

## Features

* 🔌 **Plug-and-play:** `SidecarV3(model, tok, ...)` — no retraining.
* 📦 **Constant footprint:** low-rank (U,V) fast weights + decay.
* 🧠 **Human-ish gating:** NLL + novelty + arousal (+ conflict/reward).
* 🪢 **Eligibility traces:** catch multi-utterance moments.
* 🎯 **Deterministic recall:** KV decode with allowed-tokens mask.
* 💾 **Persistence:** `save_state()` / `load_state()` to disk.
* 🛡️ **Device-safe & cold-start guarded** writes.

## What this notebook includes

* `sidecar_memory.py` (v0.3.3) with the affect-gated sidecar.
* A demo: *boring lunch* vs *Death Valley 117°F warning* (commit diverges).
* A tiny `SimpleFactStore` and one-word “favorite color” recall across sessions.

## Quickstart

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from sidecar_memory import SidecarV3

tok = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2")
mem = SidecarV3(model, tok, device="cuda" if torch.cuda.is_available() else "cpu")

# generate with memory in the loop
print(mem.generate("Hello!"))

# deterministic fact recall (KV + lexicon mask)
mem_text = "The user's favorite color is blue.\n"
ans = mem.generate_kv(mem_text, "Answer with just the color.", max_new_tokens=4,
                      allowed_words=["blue","red","green","yellow"])
print(ans)

# persist across sessions
mem.save_state("./demo_mem_state_v3")
# ... later ...
mem.load_state("./demo_mem_state_v3")
```

## Key knobs

* `affect_weights=(wS,wN,wA,wC,wR)` — how much S/N/A/C/R contribute.
* `warmup_tau`, `min_events` — stricter early gating.
* `rank`, `key_topk`, `decay`, `eta0`, `temp` — capacity & dynamics.
* `s_mode={'blend','nll','novelty'}`, `s_alpha` — surprisal/novelty mix.

## Roadmap

* ✅ Reward/RPE hook to emulate dopamine tagging.
* ✅ Consolidation/replay pass for longer-term stability.
* ✅ Vision key-path (CLIP/ViT) + visual arousal (brightness/contrast/anomaly).
* ✅ Unit-aware outlier priors beyond °F/\$/mph (domain packs).

**License:** MIT.
**Pitch:** Selective, human-like memory as a bolt-on. Constant size, zero base-weight changes, works with your existing HF stack.
"""

"""
Transformer Sidecar Memory — Persistent Resonance Stack (v0.1)
----------------------------------------------------------------
A bolt-on persistence module for *any* Hugging Face-style Transformer.

Goals
- Multi-scale recurrent binding (seconds→years) without changing base weights.
- Minimal surgery: prepend a state prefix, inject retrieved memory as text, optional residual bias hooks.
- Tiny memory footprint with quantized event vectors + episodic clustering stubs.

What works now (v0.1)
- SelfStateManager: tiny GRU-based always-on recurrent core
- EventStore: quantized int8 event embeddings + cosine top-k retrieval
- Retriever: bind(query, self_state) → ANN (brute-force for now) → Hopfield-like read
- StatePrefixGenerator: turns self_state into a short, steerable textual prefix
- SidecarMemory: wrap any AutoModelForCausalLM/Tokenizer to generate with persistence

What’s stubbed (future work)
- EpisodeStore with VQ clustering and residuals
- Semantic drift via LoRA/IA3 adapters (API surface provided, no training loop here)
- KV-cache token injection (requires model-specific hooks)

Usage
-----
from transformers import AutoModelForCausalLM, AutoTokenizer
from sidecar_memory import SidecarMemory

model = AutoModelForCausalLM.from_pretrained("gpt2")
tok = AutoTokenizer.from_pretrained("gpt2")
mem = SidecarMemory(model, tok, device="cuda")

reply = mem.generate("Explain resonance calculus in 2 lines.")
print(reply)

# persist state/memory between sessions
mem.save_state("./mem_state")
...
mem.load_state("./mem_state")

License: MIT
"""
from __future__ import annotations
import os
import json
import math
import time
import random
from dataclasses import dataclass, asdict
from typing import List, Tuple, Optional, Dict, Any

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F

# -----------------------------
# Utils
# -----------------------------

def _device_of(model: nn.Module) -> torch.device:
    try:
        return next(model.parameters()).device
    except StopIteration:
        return torch.device("cpu")


def cosine_sim(a: np.ndarray, b: np.ndarray) -> float:
    denom = (np.linalg.norm(a) * np.linalg.norm(b)) + 1e-8
    return float(np.dot(a, b) / denom)


def topk_cosine(q: np.ndarray, M: np.ndarray, k: int) -> Tuple[np.ndarray, np.ndarray]:
    """Brute-force top-k cosine similarity. Returns (indices, scores)."""
    if M.size == 0:
        return np.array([], dtype=np.int64), np.array([], dtype=np.float32)
    qn = q / (np.linalg.norm(q) + 1e-8)
    Mn = M / (np.linalg.norm(M, axis=1, keepdims=True) + 1e-8)
    scores = Mn @ qn
    if k >= len(scores):
        idx = np.argsort(-scores)
    else:
        idx = np.argpartition(-scores, k)[:k]
        idx = idx[np.argsort(-scores[idx])]
    return idx.astype(np.int64), scores[idx].astype(np.float32)

# -----------------------------
# Quantization helpers (symmetric int8)
# -----------------------------

def quantize_int8(x: np.ndarray) -> Tuple[np.ndarray, float]:
    s = np.max(np.abs(x)) / 127.0 + 1e-12
    q = np.clip(np.round(x / s), -127, 127).astype(np.int8)
    return q, float(s)


def dequantize_int8(q: np.ndarray, scale: float) -> np.ndarray:
    return q.astype(np.float32) * scale

# -----------------------------
# Self-state core (tiny GRU/SSM-like)
# -----------------------------
class SelfStateManager(nn.Module):
    """Always-on low-dim recurrent core that persists across turns/sessions.

    self_state ∈ R^d_s is updated per call using GRU cell over (query_feat || readout).
    """
    def __init__(self, d_state: int = 512, d_in: int = 512):
        super().__init__()
        self.d_state = d_state
        self.cell = nn.GRUCell(d_in, d_state)
        self.ln = nn.LayerNorm(d_state)
        self.register_buffer("state", torch.zeros(1, d_state))

    @torch.no_grad()
    def reset(self):
        self.state.zero_()

    @torch.no_grad()
    def get(self) -> torch.Tensor:
        return self.state.clone()

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """x: (batch=1, d_in) -> updates internal state and returns it."""
        s = self.cell(x, self.state)
        s = self.ln(s)
        self.state.copy_(s)
        return s

# -----------------------------
# Event store (minutes→hours scale)
# -----------------------------
@dataclass
class EventMeta:
    t: float
    loss_delta: float
    surprise: float
    text_hash: int

class EventStore:
    """Quantized embedding store with brute-force cosine ANN (replace with FAISS/HNSW later)."""
    def __init__(self, d: int = 256):
        self.d = d
        self.qvecs: List[np.ndarray] = []  # int8 arrays
        self.scales: List[float] = []
        self.meta: List[EventMeta] = []

    def __len__(self):
        return len(self.qvecs)

    def add(self, e: np.ndarray, meta: EventMeta):
        assert e.shape[-1] == self.d, f"expected dim {self.d}, got {e.shape}"
        q, s = quantize_int8(e.astype(np.float32))
        self.qvecs.append(q)
        self.scales.append(s)
        self.meta.append(meta)

    def matrix(self) -> np.ndarray:
        if not self.qvecs:
            return np.zeros((0, self.d), dtype=np.float32)
        # dequantize lazily; for speed keep cached? v0.1 keeps simple
        M = np.stack([dequantize_int8(q, s) for q, s in zip(self.qvecs, self.scales)], axis=0)
        return M

    def topk(self, key: np.ndarray, k: int = 16) -> Tuple[np.ndarray, np.ndarray]:
        M = self.matrix()
        return topk_cosine(key, M, k)

    def save(self, path: str):
        os.makedirs(path, exist_ok=True)
        # store qvecs and scales as npy for compactness
        np.save(os.path.join(path, "qvecs.npy"), np.stack(self.qvecs, axis=0) if self.qvecs else np.zeros((0, self.d), np.int8))
        np.save(os.path.join(path, "scales.npy"), np.array(self.scales, dtype=np.float32))
        with open(os.path.join(path, "meta.json"), "w") as f:
            json.dump([asdict(m) for m in self.meta], f)

    def load(self, path: str):
        qv = np.load(os.path.join(path, "qvecs.npy"))
        sc = np.load(os.path.join(path, "scales.npy"))
        with open(os.path.join(path, "meta.json"), "r") as f:
            meta = [EventMeta(**m) for m in json.load(f)]
        self.qvecs = [qv[i].astype(np.int8) for i in range(qv.shape[0])]
        self.scales = [float(x) for x in sc.tolist()]
        self.meta = meta

# -----------------------------
# Retriever + Hopfield-like read
# -----------------------------
class Retriever(nn.Module):
    def __init__(self, d_model: int, d_key: int = 256, d_state: int = 512, n_mem_tokens: int = 16):
        super().__init__()
        self.n_mem_tokens = n_mem_tokens
        self.q_proj = nn.Sequential(
            nn.Linear(d_model, d_key), nn.Tanh()
        )
        self.bind_mlp = nn.Sequential(
            nn.Linear(d_key + d_state, d_key), nn.ReLU(), nn.Linear(d_key, d_key)
        )
        # Memory token generator: project aggregated readout back to token-embedding space later
        self.readout_proj = nn.Sequential(
            nn.Linear(d_key, d_key), nn.Tanh()
        )

    def make_key(self, q_feat: torch.Tensor, self_state: torch.Tensor) -> torch.Tensor:
        qk = self.q_proj(q_feat)  # (1, d_key)
        key = self.bind_mlp(torch.cat([qk, self_state], dim=-1))
        key = F.normalize(key, dim=-1)
        return key  # (1, d_key)

    @torch.no_grad()
    def hopfield_read(self, key: torch.Tensor, candidates: np.ndarray) -> torch.Tensor:
        """Classic attention over candidate vectors, returns a single readout."""
        if candidates.size == 0:
            return key  # nothing found → fall back to key itself
        C = torch.from_numpy(candidates).float().to(key.device)  # (K, d_key)
        k = key[0]  # (d_key,)
        sims = F.normalize(C, dim=-1) @ F.normalize(k, dim=-1)
        w = F.softmax(sims * 8.0, dim=0)  # temperature=1/8
        r = (w[:, None] * C).sum(dim=0, keepdim=True)  # (1, d_key)
        return self.readout_proj(r)

# -----------------------------
# State prefix generator (text-side steering)
# -----------------------------
class StatePrefixGenerator(nn.Module):
    def __init__(self, d_state: int, max_tokens: int = 32):
        super().__init__()
        self.max_tokens = max_tokens
        self.to_tags = nn.Sequential(
            nn.Linear(d_state, 128), nn.Tanh(), nn.Linear(128, 6)
        )
        self.tag_vocab = [
            "calm", "curious", "precise", "pragmatic", "creative", "cautious"
        ]

    @torch.no_grad()
    def make_prefix(self, self_state: torch.Tensor) -> str:
        logits = self.to_tags(self_state)  # (1, 6)
        probs = torch.softmax(logits, dim=-1)[0].cpu().numpy()
        top = np.argsort(-probs)[:3]
        tags = [self.tag_vocab[i] for i in top]
        # Compact textual prefix (can be improved with learned small LM)
        return (
            f"[STATE tags={','.join(tags)} intent=helpful memory=enabled]\n"
        )

# -----------------------------
# Sidecar wrapper
# -----------------------------
class SidecarMemory:
    def __init__(
        self,
        model: nn.Module,
        tokenizer,
        device: str | torch.device = "cpu",
        d_model: int = 768,
        d_state: int = 512,
        d_key: int = 256,
        n_mem_tokens: int = 16,
    ):
        self.model = model.to(device)
        self.tok = tokenizer
        self.device = torch.device(device)

        self.self_state = SelfStateManager(d_state=d_state, d_in=d_key).to(self.device)
        self.retriever = Retriever(d_model=d_model, d_key=d_key, d_state=d_state, n_mem_tokens=n_mem_tokens).to(self.device)
        self.prefixer = StatePrefixGenerator(d_state=d_state)
        self.events = EventStore(d=d_key)

        # Heuristic caps
        self.max_event_store = 100_000
        self.keep_fraction = 0.6  # keep top 60% by surprise if over capacity

    # -------------------------
    # Encoding helpers
    # -------------------------
    @torch.no_grad()
    def _encode_for_query(self, prompt: str, max_len: int = 256) -> torch.Tensor:
        tokens = self.tok(prompt, return_tensors="pt", truncation=True, max_length=max_len).to(self.device)
        out = self.model.transformer(**tokens) if hasattr(self.model, "transformer") else self.model.base_model(**tokens)
        h = out.last_hidden_state  # (1, T, d)
        q = h[:, -1, :]  # last token representation
        return q  # (1, d_model)

    @torch.no_grad()
    def _pool_hidden(self, hidden: torch.Tensor) -> torch.Tensor:
        return hidden.mean(dim=1)  # (1, d_model)

    # -------------------------
    # Memory write
    # -------------------------
    @torch.no_grad()
    def write_event(self, context: str, loss_delta: float = 0.0, surprise: float = 0.0):
        q_feat = self._encode_for_query(context)  # (1, d_model)
        with torch.no_grad():
            key = self.retriever.q_proj(q_feat).cpu().numpy()[0]
        meta = EventMeta(t=time.time(), loss_delta=float(loss_delta), surprise=float(surprise), text_hash=hash(context) & 0xFFFFFFFF)
        self.events.add(key, meta)
        # Capacity control
        if len(self.events) > self.max_event_store:
            # drop lowest surprise fraction
            order = np.argsort([m.surprise for m in self.events.meta])[::-1]
            keep_n = int(self.keep_fraction * len(order))
            keep_idx = set(order[:keep_n].tolist())
            self.events.qvecs = [self.events.qvecs[i] for i in range(len(order)) if i in keep_idx]
            self.events.scales = [self.events.scales[i] for i in range(len(order)) if i in keep_idx]
            self.events.meta = [self.events.meta[i] for i in range(len(order)) if i in keep_idx]

    # -------------------------
    # Retrieval + assembly
    # -------------------------
    @torch.no_grad()
    def _retrieve_snippets(self, query: str, k: int = 16) -> Tuple[List[str], torch.Tensor]:
        q_feat = self._encode_for_query(query)  # (1, d_model)
        key = self.retriever.make_key(q_feat, self.self_state.get().to(self.device))  # (1, d_key)
        idx, _scores = self.events.topk(key.cpu().numpy()[0], k=k)
        candidates = self.events.matrix()[idx] if len(idx) else np.zeros((0, self.events.d), dtype=np.float32)
        readout = self.retriever.hopfield_read(key, candidates)  # (1, d_key)
        # For now, we only return minimal textual snippets (placeholders)
        snippets = []
        for i in idx[:8]:
            m = self.events.meta[int(i)]
            # A compact citation-like stub; real impl would store short summaries
            snippets.append(f"[mem t={int(m.t)} surprise={m.surprise:.2f}]")
        return snippets, readout

    @torch.no_grad()
    def _assemble_prompt(self, user_prompt: str) -> Tuple[str, torch.Tensor]:
        prefix = self.prefixer.make_prefix(self.self_state.get())
        snippets, readout = self._retrieve_snippets(user_prompt)
        mem_block = ("\n".join(snippets) + "\n") if snippets else ""
        assembled = f"{prefix}{mem_block}{user_prompt}"
        return assembled, readout

    # -------------------------
    # Generate
    # -------------------------
    @torch.no_grad()
    def generate(self, prompt: str, max_new_tokens: int = 256, temperature: float = 0.7, top_p: float = 0.9) -> str:
        assembled, readout = self._assemble_prompt(prompt)
        # Update self-state with bound readout
        self.self_state(readout.to(self.device))
        inputs = self.tok(assembled, return_tensors="pt").to(self.device)
        out = self.model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=True, temperature=temperature, top_p=top_p, pad_token_id=self.tok.eos_token_id)
        text = self.tok.decode(out[0], skip_special_tokens=True)
        # Write back event with simple surprise proxy (length / temperature)
        surprise = min(10.0, len(text) / max_new_tokens * (1.0 + (1.0 - temperature)))
        self.write_event(assembled, loss_delta=0.0, surprise=surprise)
        return text

    # -------------------------
    # Persistence
    # -------------------------
    def save_state(self, path: str):
        os.makedirs(path, exist_ok=True)
        # self-state
        torch.save(self.self_state.state.cpu(), os.path.join(path, "self_state.pt"))
        # events
        events_path = os.path.join(path, "events")
        self.events.save(events_path)
        # config
        cfg = {
            "d_state": int(self.self_state.d_state),
            "d_key": int(self.events.d),
        }
        with open(os.path.join(path, "config.json"), "w") as f:
            json.dump(cfg, f)

    def load_state(self, path: str):
        s = torch.load(os.path.join(path, "self_state.pt"), map_location=self.device)
        self.self_state.state.copy_(s)
        self.events.load(os.path.join(path, "events"))

# -----------------------------
# Optional: semantic adapter API surface (stubs)
# -----------------------------
class SemanticAdapterManager:
    """Placeholder for LoRA/IA3 integration. In v0.1, this is a no-op.
    The idea: during idle windows, fine-tune small adapters on curated episodes and then freeze.
    """
    def __init__(self):
        self.adapters: Dict[str, Any] = {}

    def train_on_minibatch(self, batch_texts: List[str]):
        pass

    def enable(self):
        pass

    def disable(self):
        pass

# -----------------------------
# End of file
# -----------------------------

# Sidecar Memory — Full Demo (device-hardened) + SimpleFactStore (v0.1.3)
# Overwrites `sidecar_memory.py`, force‑reloads the module, and runs a persistence demo
# with robust one‑word recall (newline stop + bracket ban + post‑clean).

from textwrap import dedent

# ==============================
# 1) sidecar_memory.py (v0.1.3, device-hardened + decode controls)
# ==============================
sidecar_code = dedent(r'''
from __future__ import annotations
import os, json, time
from dataclasses import dataclass, asdict
from typing import List, Tuple, Dict, Any
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F

__version__ = "0.1.3"

# ---------- device helpers ----------

def mod_device(mod: nn.Module):
    try:
        return next(mod.parameters()).device
    except StopIteration:
        for _, b in mod.named_buffers():
            return b.device
        return torch.device('cpu')

def to_mod(x: torch.Tensor, mod: nn.Module):
    return x.to(mod_device(mod))

# ---------- utils ----------

def _get_base_module(model: nn.Module):
    return getattr(model, 'transformer', None) or getattr(model, 'base_model', None) or model

def topk_cosine(q: np.ndarray, M: np.ndarray, k: int):
    if M.size == 0:
        return np.array([], dtype=np.int64), np.array([], dtype=np.float32)
    qn = q / (np.linalg.norm(q) + 1e-8)
    Mn = M / (np.linalg.norm(M, axis=1, keepdims=True) + 1e-8)
    scores = Mn @ qn
    if k >= len(scores):
        idx = np.argsort(-scores)
    else:
        idx = np.argpartition(-scores, k)[:k]
        idx = idx[np.argsort(-scores[idx])]
    return idx.astype(np.int64), scores[idx].astype(np.float32)

# ---------- quant ----------

def quantize_int8(x: np.ndarray):
    s = np.max(np.abs(x)) / 127.0 + 1e-12
    q = np.clip(np.round(x / s), -127, 127).astype(np.int8)
    return q, float(s)

def dequantize_int8(q: np.ndarray, scale: float) -> np.ndarray:
    return q.astype(np.float32) * scale

# ---------- self-state ----------
class SelfStateManager(nn.Module):
    def __init__(self, d_state: int = 512, d_in: int = 256):
        super().__init__()
        self.d_state = d_state
        self.cell = nn.GRUCell(d_in, d_state)
        self.ln = nn.LayerNorm(d_state)
        self.register_buffer("state", torch.zeros(1, d_state))
    @torch.no_grad()
    def reset(self):
        self.state.zero_()
    @torch.no_grad()
    def get(self):
        return self.state.to(mod_device(self))
    def forward(self, x: torch.Tensor):
        s = self.cell(to_mod(x, self), to_mod(self.state, self))
        s = self.ln(s)
        if self.state.device != s.device:
            self.state.data = s.detach().clone()
        else:
            self.state.copy_(s)
        return self.state

# ---------- event store ----------
@dataclass
class EventMeta:
    t: float
    loss_delta: float
    surprise: float
    text_hash: int

class EventStore:
    def __init__(self, d: int = 256):
        self.d = d
        self.qvecs: List[np.ndarray] = []
        self.scales: List[float] = []
        self.meta: List[EventMeta] = []
    def __len__(self):
        return len(self.qvecs)
    def add(self, e: np.ndarray, meta: EventMeta):
        q, s = quantize_int8(e.astype(np.float32))
        self.qvecs.append(q); self.scales.append(s); self.meta.append(meta)
    def matrix(self) -> np.ndarray:
        if not self.qvecs:
            return np.zeros((0, self.d), dtype=np.float32)
        return np.stack([dequantize_int8(q, s) for q, s in zip(self.qvecs, self.scales)], axis=0)
    def topk(self, key: np.ndarray, k: int = 16):
        M = self.matrix(); return topk_cosine(key, M, k)
    def save(self, path: str):
        os.makedirs(path, exist_ok=True)
        np.save(os.path.join(path, "qvecs.npy"), np.stack(self.qvecs, axis=0) if self.qvecs else np.zeros((0, self.d), np.int8))
        np.save(os.path.join(path, "scales.npy"), np.array(self.scales, dtype=np.float32))
        with open(os.path.join(path, "meta.json"), "w") as f:
            json.dump([asdict(m) for m in self.meta], f)
    def load(self, path: str):
        qv = np.load(os.path.join(path, "qvecs.npy"))
        sc = np.load(os.path.join(path, "scales.npy"))
        with open(os.path.join(path, "meta.json"), "r") as f:
            meta = [EventMeta(**m) for m in json.load(f)]
        self.qvecs = [qv[i].astype(np.int8) for i in range(qv.shape[0])]
        self.scales = [float(x) for x in sc.tolist()]
        self.meta = meta

# ---------- retriever ----------
class Retriever(nn.Module):
    def __init__(self, d_model: int, d_key: int = 256, d_state: int = 512):
        super().__init__()
        self.q_proj = nn.Sequential(nn.Linear(d_model, d_key), nn.Tanh())
        self.bind_mlp = nn.Sequential(nn.Linear(d_key + d_state, d_key), nn.ReLU(), nn.Linear(d_key, d_key))
        self.readout_proj = nn.Sequential(nn.Linear(d_key, d_key), nn.Tanh())
    def make_key(self, q_feat: torch.Tensor, self_state: torch.Tensor):
        qk = self.q_proj(to_mod(q_feat, self.q_proj))
        key = self.bind_mlp(torch.cat([qk, to_mod(self_state, self.bind_mlp)], dim=-1))
        return F.normalize(key, dim=-1)
    @torch.no_grad()
    def hopfield_read(self, key: torch.Tensor, candidates: np.ndarray):
        if candidates.size == 0:
            return key
        C = torch.from_numpy(candidates).float().to(mod_device(self.readout_proj))
        k = to_mod(key[0], self.readout_proj)
        sims = F.normalize(C, dim=-1) @ F.normalize(k, dim=-1)
        w = F.softmax(sims * 8.0, dim=0)
        r = (w[:, None] * C).sum(dim=0, keepdim=True)
        return self.readout_proj(r)

# ---------- prefix ----------
class StatePrefixGenerator(nn.Module):
    def __init__(self, d_state: int, max_tokens: int = 32):
        super().__init__()
        self.max_tokens = max_tokens
        self.to_tags = nn.Sequential(nn.Linear(d_state, 128), nn.Tanh(), nn.Linear(128, 6))
        self.tag_vocab = ["calm","curious","precise","pragmatic","creative","cautious"]
    @torch.no_grad()
    def make_prefix(self, self_state: torch.Tensor) -> str:
        logits = self.to_tags(to_mod(self_state, self.to_tags))
        probs = torch.softmax(logits, dim=-1)[0].detach().cpu().numpy()
        top = np.argsort(-probs)[:3]
        tags = [self.tag_vocab[i] for i in top]
        return f"[STATE tags={','.join(tags)} intent=helpful memory=enabled]\n"

# ---------- sidecar ----------
class SidecarMemory:
    def __init__(self, model, tokenizer, device="cpu", d_model=None, d_state=512, d_key=256):
        self.model = model.to(device)
        self.tok = tokenizer
        self.device = torch.device(device)
        if d_model is None:
            d_model = getattr(getattr(model, 'config', None), 'n_embd', 768)
        self.self_state = SelfStateManager(d_state=d_state, d_in=d_key).to(self.device)
        self.retriever = Retriever(d_model=d_model, d_key=d_key, d_state=d_state).to(self.device)
        self.prefixer = StatePrefixGenerator(d_state=d_state).to(self.device)
        self.events = EventStore(d=d_key)
        self.max_event_store = 100_000
        self.keep_fraction = 0.6
    @torch.no_grad()
    def _encode_for_query(self, prompt: str, max_len: int = 256):
        tokens = self.tok(prompt, return_tensors="pt", truncation=True, max_length=max_len).to(self.device)
        base = _get_base_module(self.model)
        out = base(**tokens)
        h = out.last_hidden_state
        return h[:, -1, :]
    @torch.no_grad()
    def write_event(self, context: str, loss_delta: float = 0.0, surprise: float = 0.0):
        q_feat = self._encode_for_query(context)
        key = self.retriever.q_proj(to_mod(q_feat, self.retriever)).detach().cpu().numpy()[0]
        meta = EventMeta(t=time.time(), loss_delta=float(loss_delta), surprise=float(surprise), text_hash=hash(context)&0xFFFFFFFF)
        self.events.add(key, meta)
        if len(self.events) > self.max_event_store:
            order = np.argsort([m.surprise for m in self.events.meta])[::-1]
            keep_n = int(self.keep_fraction * len(order))
            keep_idx = set(order[:keep_n].tolist())
            self.events.qvecs = [self.events.qvecs[i] for i in range(len(order)) if i in keep_idx]
            self.events.scales = [self.events.scales[i] for i in range(len(order)) if i in keep_idx]
            self.events.meta = [self.events.meta[i] for i in range(len(order)) if i in keep_idx]
    @torch.no_grad()
    def _retrieve_snippets(self, query: str, k: int = 16):
        q_feat = self._encode_for_query(query)
        key = self.retriever.make_key(q_feat, self.self_state.get())
        idx, _ = self.events.topk(key.detach().cpu().numpy()[0], k=k)
        candidates = self.events.matrix()[idx] if len(idx) else np.zeros((0, self.events.d), dtype=np.float32)
        readout = self.retriever.hopfield_read(key, candidates)
        snippets = []
        for i in idx[:8]:
            m = self.events.meta[int(i)]
            snippets.append(f"[mem t={int(m.t)} surprise={m.surprise:.2f}]")
        return snippets, readout
    @torch.no_grad()
    def _assemble_prompt(self, user_prompt: str, fact_block: str = ""):
        prefix = self.prefixer.make_prefix(self.self_state.get())
        snippets, readout = self._retrieve_snippets(user_prompt)
        mem_block = ("\n".join(snippets) + "\n") if snippets else ""
        assembled = f"{prefix}{fact_block}{mem_block}{user_prompt}"
        return assembled, readout
    @torch.no_grad()
    def generate(self, prompt: str, max_new_tokens: int = 128, temperature: float = 0.7, top_p: float = 0.9, fact_block: str = "", do_sample: bool | None = None, ban_brackets: bool = False, stop_on_newline: bool = False):
        assembled, readout = self._assemble_prompt(prompt, fact_block=fact_block)
        self.self_state(readout)
        inputs = self.tok(assembled, return_tensors="pt").to(self.device)

        # Choose sampling strategy safely
        if do_sample is None:
            do_sample = (temperature is not None) and (float(temperature) > 0.0)

        gen_kwargs = {
            "max_new_tokens": max_new_tokens,
            "pad_token_id": self.tok.eos_token_id,
            "use_cache": True,
        }
        if stop_on_newline:
            nl_ids = self.tok.encode("\n", add_special_tokens=False)
            if nl_ids:
                gen_kwargs["eos_token_id"] = [self.tok.eos_token_id] + nl_ids
        else:
            gen_kwargs["eos_token_id"] = self.tok.eos_token_id

        if ban_brackets:
            # block both '[' and ']' tokens (and their common BPE variants)
            bad = []
            for ch in ["[", "]"]:
                ids = self.tok.encode(ch, add_special_tokens=False)
                if ids:
                    bad.append(ids)
            if bad:
                gen_kwargs["bad_words_ids"] = bad

        if do_sample:
            gen_kwargs.update({"do_sample": True, "temperature": float(temperature), "top_p": float(top_p)})
        else:
            gen_kwargs.update({"do_sample": False})

        out = self.model.generate(**inputs, **gen_kwargs)
        # Decode only NEW tokens
        prompt_len = inputs["input_ids"].shape[1]
        gen_ids = out[0, prompt_len:]
        text = self.tok.decode(gen_ids, skip_special_tokens=True)

        surprise = min(10.0, len(text) / max_new_tokens * (1.0 + (1.0 - max(1e-6, float(temperature) if do_sample else 1.0))))
        self.write_event(assembled, loss_delta=0.0, surprise=surprise)
        return text
    def save_state(self, path: str):
        os.makedirs(path, exist_ok=True)
        torch.save(self.self_state.state.detach().cpu(), os.path.join(path, "self_state.pt"))
        self.events.save(os.path.join(path, "events"))
        with open(os.path.join(path, "config.json"), "w") as f:
            json.dump({"d_state": int(self.self_state.d_state), "d_key": int(self.events.d)}, f)
    def load_state(self, path: str):
        s = torch.load(os.path.join(path, "self_state.pt"), map_location=self.device)
        self.self_state.state.data = s.detach().to(self.device)
        self.events.load(os.path.join(path, "events"))
''')

with open('sidecar_memory.py', 'w') as f:
    f.write(sidecar_code)
print('✅ Wrote sidecar_memory.py (v0.1.3)')

# ==============================
# 2) Demo: add a SimpleFactStore for explicit facts
# ==============================
import re, json, os, sys, importlib

class SimpleFactStore:
    def __init__(self, path='./demo_mem_state/facts.json'):
        self.path = path
        self.data = {}
        self.load()
    def load(self):
        try:
            if os.path.exists(self.path):
                with open(self.path, 'r') as f:
                    self.data = json.load(f)
        except Exception:
            self.data = {}
    def save(self):
        os.makedirs(os.path.dirname(self.path), exist_ok=True)
        with open(self.path, 'w') as f:
            json.dump(self.data, f)
    def update_from_text(self, text: str):
        m = re.search(r"favorite\s+color\s+is\s+([A-Za-z]+)", text, flags=re.I)
        if m:
            self.data['favorite_color'] = m.group(1).lower()
    def render_block(self):
        if not self.data:
            return ''
        lines = [f"- {k}: {v}" for k, v in self.data.items()]
        return "[MEMORY FACTS]\n" + "\n".join(lines) + "\n"

# Force-reload sidecar module to avoid stale cached version
if 'sidecar_memory' in sys.modules:
    importlib.reload(sys.modules['sidecar_memory'])
import sidecar_memory as scm
print('sidecar_memory version:', getattr(scm, '__version__', 'unknown'))
SidecarMemory = scm.SidecarMemory

# ==============================
# 3) Run the demo
# ==============================
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
MODEL_NAME = 'gpt2'
print(f'Loading {MODEL_NAME} on {DEVICE}...')

tok = AutoTokenizer.from_pretrained(MODEL_NAME)
if tok.eos_token is None:
    tok.eos_token = tok.pad_token = tok.sep_token = '<|endoftext|>'
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(DEVICE)

mem = SidecarMemory(model, tok, device=DEVICE)
facts = SimpleFactStore()

# helper to extract a single word (prefer known colors)
KNOWN_COLORS = {
    'blue','red','green','yellow','orange','purple','pink','black','white','gray','grey','brown',
    'cyan','magenta','violet','indigo','teal','maroon','beige','silver','gold','navy','lime'
}

def extract_one_word(s: str) -> str:
    # prefer known colors
    for w in re.findall(r"[A-Za-z]+", s):
        lw = w.lower()
        if lw in KNOWN_COLORS:
            return lw
    m = re.search(r"\b([A-Za-z]{3,20})\b", s)
    return m.group(1).lower() if m else s.strip()[:20]

print('\n=== Session 1 ===')
# 1) greeting
u1 = 'Hello, who are you?'
print('USER:', u1)
r1 = mem.generate(u1, max_new_tokens=40)
print('ASSISTANT:', r1)

# 2) remember color
u2 = 'Remember that my favorite color is blue.'
print('\nUSER:', u2)
facts.update_from_text(u2); facts.save()
r2 = mem.generate(u2, max_new_tokens=40, fact_block=facts.render_block(), ban_brackets=True, stop_on_newline=True)
print('ASSISTANT:', r2)

# 3) recall (deterministic + constrained)
u3 = 'What color did I say I liked? Answer with just the color.'
print('\nUSER:', u3)
r3_raw = mem.generate(u3, max_new_tokens=3, temperature=0.0, top_p=1.0, fact_block=facts.render_block(), do_sample=False, ban_brackets=True, stop_on_newline=True)
r3 = extract_one_word(r3_raw) or facts.data.get('favorite_color','')
print('ASSISTANT:', r3)

SAVE_DIR = './demo_mem_state'
mem.save_state(SAVE_DIR)
facts.save()
print(f'💾 Saved state to {SAVE_DIR}')

print('\n=== New session (state reloaded) ===')
new_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(DEVICE)
new_mem = SidecarMemory(new_model, tok, device=DEVICE)
new_mem.load_state(SAVE_DIR)

facts2 = SimpleFactStore()
u4 = 'What color did I say I liked? Answer with just the color.'
print('USER:', u4)
r4_raw = new_mem.generate(u4, max_new_tokens=3, temperature=0.0, top_p=1.0, fact_block=facts2.render_block(), do_sample=False, ban_brackets=True, stop_on_newline=True)
r4 = extract_one_word(r4_raw) or facts2.data.get('favorite_color','')
print('ASSISTANT:', r4)

# Sidecar Memory — v0.3.2 (Better Gate + Lexicon Recall)
# Fixes from your run:
#  • Cold‑start gate is now strict (G > τ) and novelty is clamped to [0,1] via (1−cos)/2
#  • Higher warmup τ by default to reduce early false commits
#  • KV recall supports a lexicon mask (only allow color tokens) → no more "- favorite_"
#  • Demo updated accordingly

from textwrap import dedent

# ==============================
# 1) sidecar_memory.py (v0.3.2)
# ==============================
sidecar_code = dedent(r'''
from __future__ import annotations
import os, json, time, math
from dataclasses import dataclass, asdict
from typing import List, Tuple, Dict, Any, Optional
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F

__version__ = "0.3.2"

# ---------------- device helpers ----------------

def mod_device(mod: nn.Module):
    try:
        return next(mod.parameters()).device
    except StopIteration:
        for _, b in mod.named_buffers():
            return b.device
        return torch.device('cpu')

def to_mod(x: torch.Tensor, mod: nn.Module):
    return x.to(mod_device(mod))

# ---------------- utils ----------------

def _get_base_module(model: nn.Module):
    return getattr(model, 'transformer', None) or getattr(model, 'base_model', None) or model

def l2_normalize(x: torch.Tensor, eps: float = 1e-8):
    return x / (x.norm(dim=-1, keepdim=True) + eps)

def cosine_sim(a: torch.Tensor, b: torch.Tensor, eps: float = 1e-8):
    an = a / (a.norm(dim=-1, keepdim=True) + eps)
    bn = b / (b.norm(dim=-1, keepdim=True) + eps)
    return (an * bn).sum(dim=-1)

# k‑WTA sparsify (keep top_k by abs value)
@torch.no_grad()
def sparsify_topk(x: torch.Tensor, top_k: int):
    if top_k <= 0 or top_k >= x.shape[-1]:
        return x
    vals = x.abs()
    thr = torch.topk(vals, k=top_k, dim=-1).values[..., -1:]
    mask = vals >= thr
    return x * mask

# ---------------- self‑state ----------------
class SelfStateManager(nn.Module):
    def __init__(self, d_state: int = 512, d_in: int = 256):
        super().__init__()
        self.d_state = d_state
        self.cell = nn.GRUCell(d_in, d_state)
        self.ln = nn.LayerNorm(d_state)
        self.register_buffer("state", torch.zeros(1, d_state))
    @torch.no_grad()
    def reset(self):
        self.state.zero_()
    @torch.no_grad()
    def get(self):
        return self.state.to(mod_device(self))
    def forward(self, x: torch.Tensor):
        s = self.cell(to_mod(x, self), to_mod(self.state, self))
        s = self.ln(s)
        if self.state.device != s.device:
            self.state.data = s.detach().clone()
        else:
            self.state.copy_(s)
        return self.state

# ---------------- fast weights (low‑rank) ----------------
class FastWeightsMemory(nn.Module):
    """Low‑rank fast weights A ≈ U V^T with Hebbian updates, decay, and sparse keys.
    Read: r = V @ softmax(U^T k / T)
    Commit: U,V ← λ U,V + η * (k ⊗ α^T, v_resid ⊗ α^T)
    """
    def __init__(self, d: int = 256, rank: int = 64, decay: float = 0.995, eta0: float = 0.1, temp: float = 0.5, key_topk: int = 32):
        super().__init__()
        self.d, self.r = d, rank
        self.decay = decay
        self.eta0 = eta0
        self.temp = temp
        self.key_topk = key_topk
        self.register_buffer('U', torch.zeros(d, rank))
        self.register_buffer('V', torch.zeros(d, rank))
        nn.init.orthogonal_(self.U)
        nn.init.orthogonal_(self.V)

    @torch.no_grad()
    def _alpha(self, k: torch.Tensor):
        # k: (1,d)
        k = sparsify_topk(k, self.key_topk)
        logits = (self.U.t() @ k.t()).squeeze(-1) / max(1e-6, self.temp)  # (r)
        return torch.softmax(logits, dim=-1).unsqueeze(0)  # (1,r)

    @torch.no_grad()
    def read(self, k: torch.Tensor) -> torch.Tensor:
        # k: (1,d)
        a = self._alpha(k)  # (1,r)
        r = a @ self.V.t()  # (1,d)
        return r

    @torch.no_grad()
    def commit(self, k: torch.Tensor, v_resid: torch.Tensor, gain: float = 1.0):
        # decay
        self.U.mul_(self.decay)
        self.V.mul_(self.decay)
        # updates
        a = self._alpha(k)  # (1,r)
        eta = self.eta0 * float(gain)
        # Outer products
        self.U.add_(k.t() @ a)  # (d,1)@(1,r) -> (d,r)
        self.V.add_(v_resid.t() @ a)
        # scale by eta
        self.U.mul_(1.0 + (eta - 1.0))
        self.V.mul_(1.0 + (eta - 1.0))
        # stabilize
        self.U.div_(self.U.norm(dim=0, keepdim=True) + 1e-6)
        self.V.div_(self.V.norm(dim=0, keepdim=True) + 1e-6)

# ---------------- eligibility traces ----------------
@dataclass
class Elig:
    t: float
    k: torch.Tensor  # (1,d)
    v: torch.Tensor  # (1,d)
    s: float         # surprise

class EligBuffer:
    def __init__(self, half_life_s: float = 10.0, window_s: float = 30.0):
        self.h = half_life_s
        self.w = window_s
        self.buf: List[Elig] = []
    def add(self, k: torch.Tensor, v: torch.Tensor, s: float):
        self.buf.append(Elig(time.time(), k.detach().cpu(), v.detach().cpu(), float(s)))
    def pop_alive(self) -> List[Elig]:
        now = time.time()
        keep, out = [], []
        for e in self.buf:
            if now - e.t <= self.w:
                out.append(e)
            else:
                keep.append(e)
        self.buf = keep
        return out

# ---------------- surprise gate ----------------
class SurpriseGate:
    def __init__(self, wS=0.6, wN=0.4, thresh_mode='ema', ema_beta=0.9, ksigma=0.5, fixed_tau: Optional[float]=None, min_events: int = 5, warmup_tau: float = 0.98):
        self.wS, self.wN = wS, wN
        self.mode = thresh_mode
        self.beta = ema_beta
        self.ksigma = ksigma
        self.fixed_tau = fixed_tau
        self.mu = 0.3
        self.var = 0.02
        self.count = 0
        self.min_events = int(min_events)
        self.warmup_tau = float(warmup_tau)
    def score(self, S: float, N: float, R: float = 0.0) -> float:
        return self.wS*S + self.wN*N + 0.0*R
    def threshold(self) -> float:
        if self.fixed_tau is not None:
            return float(self.fixed_tau)
        if self.count < self.min_events:
            return self.warmup_tau  # cold‑start guard
        # EMA mean+ksigma*std after warmup
        return float(self.mu + self.ksigma * math.sqrt(max(1e-6, self.var)))
    def update_stats(self, G: float):
        b = self.beta
        self.mu = b*self.mu + (1-b)*G
        self.var = b*self.var + (1-b)*(G - self.mu)**2
        self.count += 1

# ---------------- prefix tags ----------------
class StatePrefixGenerator(nn.Module):
    def __init__(self, d_state: int, max_tokens: int = 32):
        super().__init__()
        self.max_tokens = max_tokens
        self.to_tags = nn.Sequential(nn.Linear(d_state, 128), nn.Tanh(), nn.Linear(128, 6))
        self.tag_vocab = ["calm","curious","precise","pragmatic","creative","cautious"]
    @torch.no_grad()
    def make_prefix(self, self_state: torch.Tensor) -> str:
        logits = self.to_tags(to_mod(self_state, self.to_tags))
        probs = torch.softmax(logits, dim=-1)[0].detach().cpu().numpy()
        top = np.argsort(-probs)[:3]
        tags = [self.tag_vocab[i] for i in top]
        return f"[STATE tags={','.join(tags)} intent=helpful memory=enabled]\n"

# ---------------- main sidecar (fast‑weights) ----------------
class SidecarV3(nn.Module):
    def __init__(self, model, tokenizer, device="cpu", d_model=None, d_key=256, d_state=512, rank=64, key_topk=32, decay=0.995, eta0=0.1, temp=0.5,
                 fixed_tau: Optional[float]=None, min_events: int = 5, warmup_tau: float = 0.98, s_mode: str = 'blend', s_alpha: float = 0.6):
        super().__init__()
        self.model = model.to(device)
        self.tok = tokenizer
        self.device = torch.device(device)
        d_model = d_model or getattr(getattr(model, 'config', None), 'n_embd', 768)
        # projections
        self.q_proj = nn.Sequential(nn.Linear(d_model, d_key), nn.Tanh()).to(self.device)
        self.self_state = SelfStateManager(d_state=d_state, d_in=d_key).to(self.device)
        self.prefixer = StatePrefixGenerator(d_state=d_state).to(self.device)
        self.fast = FastWeightsMemory(d=d_key, rank=rank, decay=decay, eta0=eta0, temp=temp, key_topk=key_topk).to(self.device)
        self.elig = EligBuffer()
        self.gate = SurpriseGate(fixed_tau=fixed_tau, min_events=min_events, warmup_tau=warmup_tau)
        self.s_mode = s_mode
        self.s_alpha = float(s_alpha)

    @torch.no_grad()
    def _encode_feat(self, text: str, max_len: int = 256):
        tokens = self.tok(text, return_tensors="pt", truncation=True, max_length=max_len).to(self.device)
        base = _get_base_module(self.model)
        out = base(**tokens)
        h = out.last_hidden_state  # (1, T, d_model)
        feat = h[:, -1, :]         # (1, d_model)
        k = self.q_proj(feat)      # (1, d_key)
        k = l2_normalize(k)
        return k

    @torch.no_grad()
    def _nll_surprise(self, text: str, max_len: int = 256) -> float:
        toks = self.tok(text, return_tensors="pt", truncation=True, max_length=max_len).to(self.device)
        labels = toks["input_ids"].clone()
        out = self.model(**toks, labels=labels)
        loss = float(out.loss.detach().cpu().item())
        return math.tanh(loss / 3.0)

    @torch.no_grad()
    def _surprise_novelty_read(self, k: torch.Tensor, text: str):
        r = self.fast.read(k)
        # novelty in [0,1]
        cos = float(cosine_sim(k, r).clamp(-1,1).item())
        N = (1.0 - cos) * 0.5
        S_nll = self._nll_surprise(text)
        if self.s_mode == 'novelty':
            S = N
        elif self.s_mode == 'nll':
            S = S_nll
        else:
            S = self.s_alpha * S_nll + (1.0 - self.s_alpha) * N
        return S, N, r

    @torch.no_grad()
    def _write(self, text: str) -> Dict[str, float]:
        k = self._encode_feat(text)
        S, N, r = self._surprise_novelty_read(k, text)
        v_resid = l2_normalize((k - r))
        G = self.gate.score(S, N)
        self.gate.update_stats(G)
        committed = 0.0
        # STRICT gate: only commit if G > τ
        if G > self.gate.threshold():
            self.fast.commit(k, v_resid, gain=G)
            for e in self.elig.pop_alive():
                self.fast.commit(e.k.to(self.device), e.v.to(self.device), gain=G)
            committed = 1.0
        else:
            self.elig.add(k, v_resid, S)
        return {"S": S, "N": N, "G": G, "tau": self.gate.threshold(), "committed": committed}

    @torch.no_grad()
    def _assemble_prompt(self, user_prompt: str):
        k = self._encode_feat(user_prompt)
        r = self.fast.read(k)
        self.self_state(r)
        prefix = self.prefixer.make_prefix(self.self_state.get())
        return prefix + user_prompt

    @torch.no_grad()
    def generate(self, prompt: str, max_new_tokens: int = 64, temperature: float = 0.7, top_p: float = 0.9):
        assembled = self._assemble_prompt(prompt)
        _ = self._write(assembled)
        inputs = self.tok(assembled, return_tensors="pt").to(self.device)
        do_sample = (temperature is not None) and (float(temperature) > 0.0)
        gen_kwargs = {"max_new_tokens": max_new_tokens, "pad_token_id": self.tok.eos_token_id, "eos_token_id": self.tok.eos_token_id, "use_cache": True, "do_sample": do_sample}
        if do_sample:
            gen_kwargs.update({"temperature": float(temperature), "top_p": float(top_p)})
        out = self.model.generate(**inputs, **gen_kwargs)
        prompt_len = inputs["input_ids"].shape[1]
        gen_ids = out[0, prompt_len:]
        return self.tok.decode(gen_ids, skip_special_tokens=True)

    @torch.no_grad()
    def _token_ids_for_words(self, words: List[str]) -> List[int]:
        ids = set()
        for w in words:
            for v in [w, w.capitalize(), f" {w}", f" {w.capitalize()}"]:
                tok = self.tok.encode(v, add_special_tokens=False)
                if len(tok) == 1:
                    ids.add(tok[0])
        return list(ids)

    @torch.no_grad()
    def generate_kv(self, mem_text: str, prompt: str, max_new_tokens: int = 16, stop_on_newline: bool = False, allowed_words: Optional[List[str]] = None):
        device = self.device
        # Build KV from memory text only (no echo in outputs)
        mem_ids = self.tok(mem_text, return_tensors="pt").input_ids.to(device)
        out = self.model(input_ids=mem_ids, use_cache=True)
        past = out.past_key_values
        # Prefill prompt
        p_ids = self.tok(prompt, return_tensors="pt").input_ids.to(device)
        out = self.model(input_ids=p_ids, past_key_values=past, use_cache=True)
        past = out.past_key_values
        logits = out.logits[:, -1, :]
        eos_id = self.tok.eos_token_id
        nl_ids = self.tok.encode("\n", add_special_tokens=False) if stop_on_newline else []

        allow = None
        if allowed_words:
            allow = set(self._token_ids_for_words(allowed_words))

        gen = []
        for _ in range(max_new_tokens):
            next_logits = logits.clone()
            if allow:
                mask = torch.full_like(next_logits, -1e9)
                mask[:, list(allow)] = next_logits[:, list(allow)]
                next_logits = mask
            nid = torch.argmax(next_logits, dim=-1)
            if int(nid) == eos_id or (nl_ids and int(nid) in nl_ids):
                break
            gen.append(int(nid))
            out = self.model(input_ids=nid.view(1,1), past_key_values=past, use_cache=True)
            past = out.past_key_values
            logits = out.logits[:, -1, :]
        return self.tok.decode(gen, skip_special_tokens=True)

    # persistence
    def save_state(self, path: str):
        os.makedirs(path, exist_ok=True)
        torch.save(self.self_state.state.detach().cpu(), os.path.join(path, "self_state.pt"))
        torch.save({"U": self.fast.U.detach().cpu(), "V": self.fast.V.detach().cpu()}, os.path.join(path, "fastweights.pt"))
        with open(os.path.join(path, "gate.json"), "w") as f:
            json.dump({"mu": self.gate.mu, "var": self.gate.var, "count": self.gate.count}, f)
    def load_state(self, path: str):
        self.self_state.state.data = torch.load(os.path.join(path, "self_state.pt"), map_location=self.device)
        fw = torch.load(os.path.join(path, "fastweights.pt"), map_location=self.device)
        self.fast.U.data.copy_(fw["U"].to(self.device))
        self.fast.V.data.copy_(fw["V"].to(self.device))
        try:
            with open(os.path.join(path, "gate.json"), "r") as f:
                g = json.load(f); self.gate.mu = float(g.get("mu", self.gate.mu)); self.gate.var = float(g.get("var", self.gate.var)); self.gate.count = int(g.get("count", self.gate.count))
        except Exception:
            pass
''')

with open('sidecar_memory.py', 'w') as f:
    f.write(sidecar_code)
print('✅ Wrote sidecar_memory.py (v0.3.2)')

# ==============================
# 2) Demo: facts + surprise gating tests (v0.3.2)
# ==============================
import re, json, os, sys, importlib, time

# Tiny fact store
class SimpleFactStore:
    def __init__(self, path='./demo_mem_state/facts.json'):
        self.path = path
        self.data = {}
        self.load()
    def load(self):
        try:
            if os.path.exists(self.path):
                with open(self.path, 'r') as f:
                    self.data = json.load(f)
        except Exception:
            self.data = {}
    def save(self):
        os.makedirs(os.path.dirname(self.path), exist_ok=True)
        with open(self.path, 'w') as f:
            json.dump(self.data, f)
    def update_from_text(self, text: str):
        m = re.search(r"favorite\s+color\s+is\s+([A-Za-z]+)", text, flags=re.I)
        if m:
            self.data['favorite_color'] = m.group(1).lower()
    def render_block(self):
        if not self.data:
            return ''
        # Simpler phrasing for KV build (reduces list‑copying behavior)
        return f"The user's favorite color is {self.data.get('favorite_color','')}\.\n"

# Force‑reload
if 'sidecar_memory' in sys.modules:
    importlib.reload(sys.modules['sidecar_memory'])
import sidecar_memory as scm
print('sidecar_memory version:', getattr(scm, '__version__', 'unknown'))
SidecarV3 = scm.SidecarV3

# ==============================
# 3) Run the demo
# ==============================
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
MODEL_NAME = 'gpt2'
print(f'Loading {MODEL_NAME} on {DEVICE}...')

tok = AutoTokenizer.from_pretrained(MODEL_NAME)
if tok.eos_token is None:
    tok.eos_token = tok.pad_token = tok.sep_token = '<|endoftext|>'
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(DEVICE)

mem = SidecarV3(
    model, tok, device=DEVICE,
    d_key=256, rank=64, key_topk=32, decay=0.997, eta0=0.15, temp=0.5,
    fixed_tau=None, min_events=4, warmup_tau=0.98, s_mode='blend', s_alpha=0.6
)
facts = SimpleFactStore()

KNOWN_COLORS = { 'blue','red','green','yellow','orange','purple','pink','black','white','gray','grey','brown','cyan','magenta','violet','indigo','teal','maroon','beige','silver','gold','navy','lime' }

def extract_one_word(s: str) -> str:
    for w in re.findall(r"[A-Za-z]+", s):
        lw = w.lower()
        if lw in KNOWN_COLORS:
            return lw
    m = re.search(r"\b([A-Za-z]{3,20})\b", s)
    return (m.group(1).lower() if m else s.strip()[:20])

print('\n=== Session 1 — Surprise gating (strict + clamped) ===')
boring = 'I ate a sandwich for lunch today. It tasted fine.'
surprising = 'I drove through Death Valley in 117 degree heat and the car thermometer flashed a warning.'

print('\nUSER:', boring)
stats1 = mem._write(boring)
print('Write stats:', stats1)
print('ASSISTANT:', mem.generate(boring, max_new_tokens=24))

print('\nUSER:', surprising)
stats2 = mem._write(surprising)
print('Write stats:', stats2)
print('ASSISTANT:', mem.generate(surprising, max_new_tokens=24))

print('\n=== Fact recall with fast‑weights in the loop ===')
teach = 'Remember that my favorite color is blue.'
print('USER:', teach)
facts.update_from_text(teach); facts.save()
print('ASSISTANT:', mem.generate(teach + "\n" + facts.render_block(), max_new_tokens=24))

ask = 'What color did I say I liked? Answer with just the color.'
print('\nUSER:', ask)
kv_ans = mem.generate_kv(facts.render_block(), ask, max_new_tokens=4, stop_on_newline=False, allowed_words=sorted(list(KNOWN_COLORS)))
print('ASSISTANT (KV):', extract_one_word(kv_ans))

SAVE_DIR = './demo_mem_state_v3'
mem.save_state(SAVE_DIR)
facts.save()
print(f'💾 Saved state to {SAVE_DIR}')

print('\n=== New session (state reloaded) ===')
new_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(DEVICE)
new_mem = SidecarV3(new_model, tok, device=DEVICE)
new_mem.load_state(SAVE_DIR)
print('USER:', ask)
kv_ans2 = new_mem.generate_kv(facts.render_block(), ask, max_new_tokens=4, stop_on_newline=False, allowed_words=sorted(list(KNOWN_COLORS)))
print('ASSISTANT (KV):', extract_one_word(kv_ans2))

# Sidecar Memory — v0.3.3 (Affect/Arousal‑Gated Fast Weights)
# Add a human‑like affect channel (arousal) to surprise gating, plus simple conflict detection.
#  • G = wS·S (NLL surprisal) + wN·N (novelty) + wA·A (affect arousal) + wC·C (conflict) + wR·R (reward)
#  • Tiny AffectScorer: hazard lexicon + numeric outliers (e.g., 117°F) + named‑entity priors + punctuation/intensifiers
#  • Strict gate with cold‑start guard; residual fast‑weights; KV recall with lexicon mask

from textwrap import dedent

# ==============================
# 1) sidecar_memory.py (v0.3.3)
# ==============================
sidecar_code = dedent(r'''
from __future__ import annotations
import os, json, time, math, re
from dataclasses import dataclass, asdict
from typing import List, Tuple, Dict, Any, Optional
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F

__version__ = "0.3.3"

# ---------------- device helpers ----------------

def mod_device(mod: nn.Module):
    try:
        return next(mod.parameters()).device
    except StopIteration:
        for _, b in mod.named_buffers():
            return b.device
        return torch.device('cpu')

def to_mod(x: torch.Tensor, mod: nn.Module):
    return x.to(mod_device(mod))

# ---------------- utils ----------------

def _get_base_module(model: nn.Module):
    return getattr(model, 'transformer', None) or getattr(model, 'base_model', None) or model

def l2_normalize(x: torch.Tensor, eps: float = 1e-8):
    return x / (x.norm(dim=-1, keepdim=True) + eps)

def cosine_sim(a: torch.Tensor, b: torch.Tensor, eps: float = 1e-8):
    an = a / (a.norm(dim=-1, keepdim=True) + eps)
    bn = b / (b.norm(dim=-1, keepdim=True) + eps)
    return (an * bn).sum(dim=-1)

@torch.no_grad()
def sparsify_topk(x: torch.Tensor, top_k: int):
    if top_k <= 0 or top_k >= x.shape[-1]:
        return x
    vals = x.abs()
    thr = torch.topk(vals, k=top_k, dim=-1).values[..., -1:]
    mask = vals >= thr
    return x * mask

# ---------------- self‑state ----------------
class SelfStateManager(nn.Module):
    def __init__(self, d_state: int = 512, d_in: int = 256):
        super().__init__()
        self.d_state = d_state
        self.cell = nn.GRUCell(d_in, d_state)
        self.ln = nn.LayerNorm(d_state)
        self.register_buffer("state", torch.zeros(1, d_state))
    @torch.no_grad()
    def reset(self):
        self.state.zero_()
    @torch.no_grad()
    def get(self):
        return self.state.to(mod_device(self))
    def forward(self, x: torch.Tensor):
        s = self.cell(to_mod(x, self), to_mod(self.state, self))
        s = self.ln(s)
        if self.state.device != s.device:
            self.state.data = s.detach().clone()
        else:
            self.state.copy_(s)
        return self.state

# ---------------- fast weights (low‑rank) ----------------
class FastWeightsMemory(nn.Module):
    """Low‑rank fast weights A ≈ U V^T with Hebbian updates, decay, and sparse keys.
    Read: r = V @ softmax(U^T k / T)
    Commit: U,V ← λ U,V + η * (k ⊗ α^T, v_resid ⊗ α^T)
    """
    def __init__(self, d: int = 256, rank: int = 64, decay: float = 0.995, eta0: float = 0.1, temp: float = 0.5, key_topk: int = 32):
        super().__init__()
        self.d, self.r = d, rank
        self.decay = decay
        self.eta0 = eta0
        self.temp = temp
        self.key_topk = key_topk
        self.register_buffer('U', torch.zeros(d, rank))
        self.register_buffer('V', torch.zeros(d, rank))
        nn.init.orthogonal_(self.U)
        nn.init.orthogonal_(self.V)
    @torch.no_grad()
    def _alpha(self, k: torch.Tensor):
        k = sparsify_topk(k, self.key_topk)
        logits = (self.U.t() @ k.t()).squeeze(-1) / max(1e-6, self.temp)
        return torch.softmax(logits, dim=-1).unsqueeze(0)
    @torch.no_grad()
    def read(self, k: torch.Tensor) -> torch.Tensor:
        a = self._alpha(k)
        return a @ self.V.t()
    @torch.no_grad()
    def commit(self, k: torch.Tensor, v_resid: torch.Tensor, gain: float = 1.0):
        self.U.mul_(self.decay); self.V.mul_(self.decay)
        a = self._alpha(k)
        eta = self.eta0 * float(gain)
        self.U.add_(k.t() @ a)
        self.V.add_(v_resid.t() @ a)
        self.U.mul_(1.0 + (eta - 1.0)); self.V.mul_(1.0 + (eta - 1.0))
        self.U.div_(self.U.norm(dim=0, keepdim=True) + 1e-6)
        self.V.div_(self.V.norm(dim=0, keepdim=True) + 1e-6)

# ---------------- eligibility traces ----------------
@dataclass
class Elig:
    t: float
    k: torch.Tensor
    v: torch.Tensor
    s: float

class EligBuffer:
    def __init__(self, half_life_s: float = 10.0, window_s: float = 30.0):
        self.h = half_life_s; self.w = window_s; self.buf: List[Elig] = []
    def add(self, k: torch.Tensor, v: torch.Tensor, s: float):
        self.buf.append(Elig(time.time(), k.detach().cpu(), v.detach().cpu(), float(s)))
    def pop_alive(self) -> List[Elig]:
        now = time.time(); keep, out = [], []
        for e in self.buf:
            if now - e.t <= self.w: out.append(e)
            else: keep.append(e)
        self.buf = keep; return out

# ---------------- affect scorer (arousal) ----------------
class AffectScorer:
    def __init__(self):
        self.hazard = set('warning flashed emergency accident police danger crash burned died death hospital icu earthquake tornado hurricane wildfire explosion evacuate flood storm alarm heatwave heatstroke blackout fire rescue stranded'.split())
        self.intens = set('very extremely unbelievably incredibly super really so highly severely dangerously insanely'.split())
        self.ne_prior = ['death valley','icu','er','emergency room','police','court','earthquake','tornado','hurricane','wildfire','war','explosion']
        self.word_re = re.compile(r"[A-Za-z']+")
        self.num_unit_re = re.compile(r"([+-]?\d+(?:\.\d+)?)\s*(°?\s*[Ff]|°?\s*[Cc]|mph|mi|miles|km|hours?|mins?|\$)?")
    def _lex_arousal(self, text: str) -> float:
        w = [t.lower() for t in self.word_re.findall(text)]
        haz = sum(1 for t in w if t in self.hazard)
        itf = sum(1 for t in w if t in self.intens)
        caps = sum(1 for t in re.findall(r"\b[A-Z]{3,}\b", text))
        excl = text.count('!')
        val = 0.15*min(3,haz) + 0.1*min(3,itf) + 0.05*min(3,caps) + 0.05*min(3,excl)
        return max(0.0, min(1.0, val))
    def _ne_arousal(self, text: str) -> float:
        tl = text.lower()
        hit = any(ne in tl for ne in self.ne_prior)
        return 0.25 if hit else 0.0
    def _num_arousal(self, text: str) -> float:
        a = 0.0
        for m in self.num_unit_re.finditer(text):
            if not m.group(1):
                continue
            x = float(m.group(1)); unit = (m.group(2) or '').lower().replace(' ', '')
            def sig(z):
                return 1/(1+math.exp(-z))
            if unit in ['°f','f','°f'] or 'f'==unit:
                a = max(a, sig((x-95)/5))
            elif unit in ['°c','c','°c'] or 'c'==unit:
                a = max(a, sig((x-35)/3))
            elif unit == 'mph':
                a = max(a, sig((x-80)/10))
            elif unit in ['mi','miles','km']:
                a = max(a, sig((x-300)/80))
            elif unit in ['hour','hours','mins','min']:
                a = max(a, sig((x-8)/2))
            elif unit == '$' or '$' in m.group(0):
                a = max(a, sig((math.log10(max(1.0,x)) - 3)/0.6))
            else:
                # generic z vs recent history could go here; keep simple
                pass
        return max(0.0, min(1.0, a))
    def arousal(self, text: str) -> float:
        a = self._lex_arousal(text) + self._ne_arousal(text) + self._num_arousal(text)
        return max(0.0, min(1.0, a))

# ---------------- surprise gate ----------------
class SurpriseGate:
    def __init__(self, wS=0.3, wN=0.15, wA=0.5, wC=0.05, wR=0.0, ema_beta=0.9, ksigma=0.5, fixed_tau: Optional[float]=None, min_events: int = 2, warmup_tau: float = 0.90):
        self.wS, self.wN, self.wA, self.wC, self.wR = wS, wN, wA, wC, wR
        self.beta = ema_beta; self.ksigma = ksigma
        self.fixed_tau = fixed_tau; self.mu = 0.3; self.var = 0.02
        self.count = 0; self.min_events = int(min_events); self.warmup_tau = float(warmup_tau)
    def score(self, S: float, N: float, A: float, C: float = 0.0, R: float = 0.0) -> float:
        return self.wS*S + self.wN*N + self.wA*A + self.wC*C + self.wR*R
    def threshold(self) -> float:
        if self.fixed_tau is not None:
            return float(self.fixed_tau)
        if self.count < self.min_events:
            return self.warmup_tau
        return float(self.mu + self.ksigma * math.sqrt(max(1e-6, self.var)))
    def update_stats(self, G: float):
        b = self.beta; self.mu = b*self.mu + (1-b)*G; self.var = b*self.var + (1-b)*(G - self.mu)**2; self.count += 1

# ---------------- prefix tags ----------------
class StatePrefixGenerator(nn.Module):
    def __init__(self, d_state: int, max_tokens: int = 32):
        super().__init__()
        self.max_tokens = max_tokens
        self.to_tags = nn.Sequential(nn.Linear(d_state, 128), nn.Tanh(), nn.Linear(128, 6))
        self.tag_vocab = ["calm","curious","precise","pragmatic","creative","cautious"]
    @torch.no_grad()
    def make_prefix(self, self_state: torch.Tensor) -> str:
        logits = self.to_tags(to_mod(self_state, self.to_tags))
        probs = torch.softmax(logits, dim=-1)[0].detach().cpu().numpy()
        top = np.argsort(-probs)[:3]
        tags = [self.tag_vocab[i] for i in top]
        return f"[STATE tags={','.join(tags)} intent=helpful memory=enabled]\n"

# ---------------- main sidecar (fast‑weights + affect) ----------------
class SidecarV3(nn.Module):
    def __init__(self, model, tokenizer, device="cpu", d_model=None, d_key=256, d_state=512, rank=64, key_topk=32, decay=0.995, eta0=0.1, temp=0.5,
                 affect_weights=(0.3,0.15,0.5,0.05,0.0), fixed_tau: Optional[float]=None, min_events: int = 2, warmup_tau: float = 0.90, s_mode: str = 'blend', s_alpha: float = 0.6):
        super().__init__()
        self.model = model.to(device); self.tok = tokenizer; self.device = torch.device(device)
        d_model = d_model or getattr(getattr(model, 'config', None), 'n_embd', 768)
        self.q_proj = nn.Sequential(nn.Linear(d_model, d_key), nn.Tanh()).to(self.device)
        self.self_state = SelfStateManager(d_state=d_state, d_in=d_key).to(self.device)
        self.prefixer = StatePrefixGenerator(d_state=d_state).to(self.device)
        self.fast = FastWeightsMemory(d=d_key, rank=rank, decay=decay, eta0=eta0, temp=temp, key_topk=key_topk).to(self.device)
        self.elig = EligBuffer(); self.affect = AffectScorer()
        wS,wN,wA,wC,wR = affect_weights
        self.gate = SurpriseGate(wS=wS,wN=wN,wA=wA,wC=wC,wR=wR,fixed_tau=fixed_tau,min_events=min_events,warmup_tau=warmup_tau)
        self.s_mode = s_mode; self.s_alpha = float(s_alpha)
        self.known_facts: Dict[str,str] = {}
        self.fact_color_re = re.compile(r"favorite\s+color\s+is\s+([A-Za-z]+)", re.I)
    @torch.no_grad()
    def update_known_facts_from_text(self, text: str):
        m = self.fact_color_re.search(text)
        if m:
            self.known_facts['favorite_color'] = m.group(1).lower()
    @torch.no_grad()
    def _encode_feat(self, text: str, max_len: int = 256):
        tokens = self.tok(text, return_tensors="pt", truncation=True, max_length=max_len).to(self.device)
        base = _get_base_module(self.model); out = base(**tokens); h = out.last_hidden_state
        k = self.q_proj(h[:, -1, :])
        return l2_normalize(k)
    @torch.no_grad()
    def _nll_surprise(self, text: str, max_len: int = 256) -> float:
        toks = self.tok(text, return_tensors="pt", truncation=True, max_length=max_len).to(self.device)
        labels = toks["input_ids"].clone(); out = self.model(**toks, labels=labels)
        loss = float(out.loss.detach().cpu().item()); return math.tanh(loss / 3.0)
    @torch.no_grad()
    def _surprise_novelty_read(self, k: torch.Tensor, text: str):
        r = self.fast.read(k); cos = float(cosine_sim(k, r).clamp(-1,1).item()); N = (1.0 - cos) * 0.5
        S_nll = self._nll_surprise(text)
        if self.s_mode == 'novelty': S = N
        elif self.s_mode == 'nll': S = S_nll
        else: S = self.s_alpha * S_nll + (1.0 - self.s_alpha) * N
        return S, N, r
    @torch.no_grad()
    def _conflict(self, text: str) -> float:
        m = self.fact_color_re.search(text)
        if m:
            new = m.group(1).lower(); old = self.known_facts.get('favorite_color')
            if old and old != new:
                return 1.0
        return 0.0
    @torch.no_grad()
    def _write(self, text: str) -> Dict[str, float]:
        k = self._encode_feat(text)
        S, N, r = self._surprise_novelty_read(k, text)
        A = float(self.affect.arousal(text))
        C = self._conflict(text)
        v_resid = l2_normalize((k - r))
        G = self.gate.score(S, N, A, C, 0.0)
        self.gate.update_stats(G)
        committed = 0.0
        if G > self.gate.threshold():
            self.fast.commit(k, v_resid, gain=G)
            for e in self.elig.pop_alive():
                self.fast.commit(e.k.to(self.device), e.v.to(self.device), gain=G)
            committed = 1.0
        else:
            self.elig.add(k, v_resid, S)
        # update internal facts after deciding commit (so conflict reflects prior state)
        self.update_known_facts_from_text(text)
        return {"S": S, "N": N, "A": A, "C": C, "G": G, "tau": self.gate.threshold(), "committed": committed}
    @torch.no_grad()
    def _assemble_prompt(self, user_prompt: str):
        k = self._encode_feat(user_prompt); r = self.fast.read(k); self.self_state(r)
        return self.prefixer.make_prefix(self.self_state.get()) + user_prompt
    @torch.no_grad()
    def generate(self, prompt: str, max_new_tokens: int = 64, temperature: float = 0.7, top_p: float = 0.9):
        assembled = self._assemble_prompt(prompt); _ = self._write(assembled)
        inputs = self.tok(assembled, return_tensors="pt").to(self.device)
        do_sample = (temperature is not None) and (float(temperature) > 0.0)
        gen_kwargs = {"max_new_tokens": max_new_tokens, "pad_token_id": self.tok.eos_token_id, "eos_token_id": self.tok.eos_token_id, "use_cache": True, "do_sample": do_sample}
        if do_sample: gen_kwargs.update({"temperature": float(temperature), "top_p": float(top_p)})
        out = self.model.generate(**inputs, **gen_kwargs); gen_ids = out[0, inputs["input_ids"].shape[1]:]
        return self.tok.decode(gen_ids, skip_special_tokens=True)
    @torch.no_grad()
    def _token_ids_for_words(self, words: List[str]) -> List[int]:
        ids = set()
        for w in words:
            for v in [w, w.capitalize(), f" {w}", f" {w.capitalize()}"]:
                tok = self.tok.encode(v, add_special_tokens=False)
                if len(tok) == 1: ids.add(tok[0])
        return list(ids)
    @torch.no_grad()
    def generate_kv(self, mem_text: str, prompt: str, max_new_tokens: int = 16, stop_on_newline: bool = False, allowed_words: Optional[List[str]] = None):
        device = self.device
        mem_ids = self.tok(mem_text, return_tensors="pt").input_ids.to(device)
        out = self.model(input_ids=mem_ids, use_cache=True); past = out.past_key_values
        p_ids = self.tok(prompt, return_tensors="pt").input_ids.to(device)
        out = self.model(input_ids=p_ids, past_key_values=past, use_cache=True); past = out.past_key_values; logits = out.logits[:, -1, :]
        eos_id = self.tok.eos_token_id; nl_ids = self.tok.encode("\n", add_special_tokens=False) if stop_on_newline else []
        allow = None
        if allowed_words: allow = set(self._token_ids_for_words(allowed_words))
        gen = []
        for _ in range(max_new_tokens):
            next_logits = logits.clone()
            if allow:
                mask = torch.full_like(next_logits, -1e9); idxs = list(allow)
                mask[:, idxs] = next_logits[:, idxs]; next_logits = mask
            nid = torch.argmax(next_logits, dim=-1)
            if int(nid) == eos_id or (nl_ids and int(nid) in nl_ids): break
            gen.append(int(nid))
            out = self.model(input_ids=nid.view(1,1), past_key_values=past, use_cache=True)
            past = out.past_key_values; logits = out.logits[:, -1, :]
        return self.tok.decode(gen, skip_special_tokens=True)
    def save_state(self, path: str):
        os.makedirs(path, exist_ok=True)
        torch.save(self.self_state.state.detach().cpu(), os.path.join(path, "self_state.pt"))
        torch.save({"U": self.fast.U.detach().cpu(), "V": self.fast.V.detach().cpu()}, os.path.join(path, "fastweights.pt"))
        with open(os.path.join(path, "gate.json"), "w") as f:
            json.dump({"mu": getattr(self.gate,'mu',0.0), "var": getattr(self.gate,'var',0.0), "count": getattr(self.gate,'count',0)}, f)
    def load_state(self, path: str):
        self.self_state.state.data = torch.load(os.path.join(path, "self_state.pt"), map_location=self.device)
        fw = torch.load(os.path.join(path, "fastweights.pt"), map_location=self.device)
        self.fast.U.data.copy_(fw["U"].to(self.device)); self.fast.V.data.copy_(fw["V"].to(self.device))
        try:
            with open(os.path.join(path, "gate.json"), "r") as f:
                g = json.load(f); self.gate.mu = float(g.get("mu", self.gate.mu)); self.gate.var = float(g.get("var", self.gate.var)); self.gate.count = int(g.get("count", self.gate.count))
        except Exception: pass
''')

with open('sidecar_memory.py', 'w') as f:
    f.write(sidecar_code)
print('✅ Wrote sidecar_memory.py (v0.3.3)')

# ==============================
# 2) Demo: affect‑gated surprise vs ordinary + color recall
# ==============================
import re, json, os, sys, importlib

class SimpleFactStore:
    def __init__(self, path='./demo_mem_state/facts.json'):
        self.path = path; self.data = {}; self.load()
    def load(self):
        try:
            if os.path.exists(self.path):
                with open(self.path, 'r') as f: self.data = json.load(f)
        except Exception: self.data = {}
    def save(self):
        os.makedirs(os.path.dirname(self.path), exist_ok=True)
        with open(self.path, 'w') as f: json.dump(self.data, f)
    def update_from_text(self, text: str):
        m = re.search(r"favorite\s+color\s+is\s+([A-Za-z]+)", text, flags=re.I)
        if m: self.data['favorite_color'] = m.group(1).lower()
    def render_block(self):
        if not self.data: return ''
        return f"The user's favorite color is {self.data.get('favorite_color','')}\.\n"

# Force‑reload
if 'sidecar_memory' in sys.modules:
    importlib.reload(sys.modules['sidecar_memory'])
import sidecar_memory as scm
print('sidecar_memory version:', getattr(scm, '__version__', 'unknown'))
SidecarV3 = scm.SidecarV3

# ==============================
# 3) Run the demo
# ==============================
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
MODEL_NAME = 'gpt2'
print(f'Loading {MODEL_NAME} on {DEVICE}...')

tok = AutoTokenizer.from_pretrained(MODEL_NAME)
if tok.eos_token is None:
    tok.eos_token = tok.pad_token = tok.sep_token = '<|endoftext|>'
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(DEVICE)

mem = SidecarV3(
    model, tok, device=DEVICE,
    d_key=256, rank=64, key_topk=32, decay=0.997, eta0=0.15, temp=0.5,
    affect_weights=(0.3,0.15,0.5,0.05,0.0), fixed_tau=None, min_events=2, warmup_tau=0.90,
    s_mode='blend', s_alpha=0.6
)
facts = SimpleFactStore()

KNOWN_COLORS = { 'blue','red','green','yellow','orange','purple','pink','black','white','gray','grey','brown','cyan','magenta','violet','indigo','teal','maroon','beige','silver','gold','navy','lime' }

def extract_one_word(s: str) -> str:
    for w in re.findall(r"[A-Za-z]+", s):
        lw = w.lower()
        if lw in KNOWN_COLORS: return lw
    m = re.search(r"\b([A-Za-z]{3,20})\b", s)
    return (m.group(1).lower() if m else s.strip()[:20])

print('\n=== Session 1 — Affect‑gated surprise ===')
boring = 'I ate a sandwich for lunch today. It tasted fine.'
surprising = 'I drove through Death Valley in 117 degree heat and the car thermometer flashed a warning.'

print('\nUSER:', boring)
stats1 = mem._write(boring)
print('Write stats:', stats1)
print('ASSISTANT:', mem.generate(boring, max_new_tokens=24))

print('\nUSER:', surprising)
stats2 = mem._write(surprising)
print('Write stats:', stats2)
print('ASSISTANT:', mem.generate(surprising, max_new_tokens=24))

print('\n=== Fact recall with fast‑weights in the loop ===')
teach = 'Remember that my favorite color is blue.'
print('USER:', teach)
facts.update_from_text(teach); facts.save(); mem.update_known_facts_from_text(teach)
print('ASSISTANT:', mem.generate(teach + "\n" + facts.render_block(), max_new_tokens=24))

ask = 'What color did I say I liked? Answer with just the color.'
print('\nUSER:', ask)
kv_ans = mem.generate_kv(facts.render_block(), ask, max_new_tokens=4, stop_on_newline=False, allowed_words=sorted(list(KNOWN_COLORS)))
print('ASSISTANT (KV):', extract_one_word(kv_ans))

SAVE_DIR = './demo_mem_state_v3'
mem.save_state(SAVE_DIR); facts.save(); print(f'💾 Saved state to {SAVE_DIR}')

print('\n=== New session (state reloaded) ===')
new_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(DEVICE)
new_mem = SidecarV3(new_model, tok, device=DEVICE)
new_mem.load_state(SAVE_DIR)
print('USER:', ask)
kv_ans2 = new_mem.generate_kv(facts.render_block(), ask, max_new_tokens=4, stop_on_newline=False, allowed_words=sorted(list(KNOWN_COLORS)))
print('ASSISTANT (KV):', extract_one_word(kv_ans2))
