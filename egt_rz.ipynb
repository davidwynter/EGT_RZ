{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üöÄ EGT Prototype ‚Äî Embedded GAN-in-Transformer (v0.3)\n",
        "\n",
        "This notebook demonstrates a **self-teaching Transformer** architecture where a **GAN** is embedded *inside* the Transformer blocks themselves.\n",
        "\n",
        "Instead of standard training, the model alternates between two modes:\n",
        "1. **Wake** ‚Äî Generates responses to prompts, scores itself with internal critic networks, and stores experiences in a replay buffer.\n",
        "2. **Sleep** ‚Äî Replays those experiences through a *GAN-like* adversarial loop *inside* the Transformer, refining both its responses and its ability to refine its own responses.\n",
        "\n",
        "### üîπ Key Innovations\n",
        "- **Embedded GAN**: Generator & Discriminators (critics) coexist inside Transformer layers.\n",
        "- **LoRA Adaptation**: Lightweight low-rank adapters for efficient continual updates.\n",
        "- **EMA Teacher Model**: Stabilizes learning over ‚Äúlifetimes.‚Äù\n",
        "- **Replay Buffer Memory**: Stores and reuses lived experiences.\n",
        "- **Artificial Sleep Cycle**: The model ‚Äúdreams‚Äù by training on its own outputs, improving its self-critique skills.\n",
        "\n",
        "### üß† Conceptual Flow\n",
        "Wake: Prompt ‚Üí Generate ‚Üí Self-Score ‚Üí Store\n",
        "Sleep: Replay ‚Üí Discriminator vs. Generator ‚Üí Teacher Alignment ‚Üí Update\n",
        "\n",
        "\n",
        "### üí° Why It‚Äôs Interesting\n",
        "This is **not** a typical ‚ÄúTransformer + GAN‚Äù pipeline.\n",
        "- The **GAN lives *inside* the Transformer module** itself.\n",
        "- The model **learns how to teach itself** better over time.\n",
        "- Inspired by biological learning patterns (wake/sleep cycles).\n",
        "\n",
        "### üìú Included Features\n",
        "- Tiny **SentencePiece-style BPE tokenizer** ‚Üí readable outputs.\n",
        "- Small demo corpus for quick tests.\n",
        "- Configurable ‚Äúdream‚Äù length and critic placement.\n",
        "- Fully runnable on CPU for proof-of-concept.\n",
        "\n",
        "---\n",
        "\n",
        "‚ö†Ô∏è **Note:** This is a research prototype, not a production-ready model.\n",
        "Scaling it up could lead to unexpected emergent behaviors ‚Äî handle responsibly."
      ],
      "metadata": {
        "id": "6B8iuegPZBlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "import math\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# =========================\n",
        "# Tiny BPE Tokenizer (SentencePiece-style)\n",
        "# =========================\n",
        "class TinyBPETokenizer:\n",
        "    \"\"\"Minimal BPE with ‚ñÅ as word boundary.\n",
        "    Trains merges on a small corpus; encodes by greedy longest-match; decodes back.\n",
        "    \"\"\"\n",
        "    def __init__(self, texts: List[str], vocab_size: int = 200):\n",
        "        self.pad = '<pad>'; self.bos = '<bos>'; self.eos = '<eos>'\n",
        "        self.special = [self.pad, self.bos, self.eos]\n",
        "        # Pre-tokenize words with leading boundary\n",
        "        words = []\n",
        "        for t in texts:\n",
        "            for w in t.strip().split():\n",
        "                if not w:\n",
        "                    continue\n",
        "                words.append('‚ñÅ' + w)\n",
        "        # Build initial vocab from characters\n",
        "        vocab = set(self.special)\n",
        "        # represent words as tuple of chars\n",
        "        corpus = []\n",
        "        for w in words:\n",
        "            syms = tuple(list(w))\n",
        "            corpus.append(syms)\n",
        "            vocab.update(syms)\n",
        "        def count_pairs(corpus_list):\n",
        "            counts: Dict[Tuple[str,str], int] = {}\n",
        "            for syms in corpus_list:\n",
        "                for i in range(len(syms)-1):\n",
        "                    pair = (syms[i], syms[i+1])\n",
        "                    counts[pair] = counts.get(pair, 0) + 1\n",
        "            return counts\n",
        "        merges_list: List[Tuple[str,str]] = []\n",
        "        # Train merges until vocab size target\n",
        "        while True:\n",
        "            tokens_so_far = {s for syms in corpus for s in syms} | set(['‚ñÅ']) | set(self.special)\n",
        "            if len(tokens_so_far) + len(merges_list) >= vocab_size:\n",
        "                break\n",
        "            pair_counts = count_pairs(corpus)\n",
        "            if not pair_counts:\n",
        "                break\n",
        "            best_pair = max(pair_counts.items(), key=lambda x: x[1])[0]\n",
        "            merges_list.append(best_pair)\n",
        "            a,b = best_pair\n",
        "            new_corpus = []\n",
        "            for syms in corpus:\n",
        "                i = 0; merged = []\n",
        "                while i < len(syms):\n",
        "                    if i < len(syms)-1 and syms[i] == a and syms[i+1] == b:\n",
        "                        merged.append(a+b); i += 2\n",
        "                    else:\n",
        "                        merged.append(syms[i]); i += 1\n",
        "                new_corpus.append(tuple(merged))\n",
        "            corpus = new_corpus\n",
        "        # Build final vocab mapping\n",
        "        tokens = self.special + sorted({s for syms in corpus for s in syms} | set(['‚ñÅ']))\n",
        "        for a,b in merges_list:\n",
        "            tok = a+b\n",
        "            if tok not in tokens:\n",
        "                tokens.append(tok)\n",
        "        self.itos = tokens\n",
        "        self.stoi = {t:i for i,t in enumerate(self.itos)}\n",
        "        self.pad_id = self.stoi[self.pad]\n",
        "        self.bos_id = self.stoi[self.bos]\n",
        "        self.eos_id = self.stoi[self.eos]\n",
        "        # Rank of merges for greedy encode\n",
        "        self.merges = {(a+b):(i+1) for i,(a,b) in enumerate(merges_list)}\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self):\n",
        "        return len(self.itos)\n",
        "\n",
        "    def encode(self, s: str, add_bos=True, add_eos=True) -> List[int]:\n",
        "        pieces: List[str] = []\n",
        "        for w in s.strip().split():\n",
        "            if not w: continue\n",
        "            syms = list('‚ñÅ' + w)\n",
        "            # Greedy merge\n",
        "            merged = syms\n",
        "            while True:\n",
        "                pair_pos = None; best_rank = None\n",
        "                for i in range(len(merged)-1):\n",
        "                    cand = merged[i] + merged[i+1]\n",
        "                    if cand in self.merges:\n",
        "                        rank = self.merges[cand]\n",
        "                        if (best_rank is None) or (rank < best_rank):\n",
        "                            best_rank = rank; pair_pos = i\n",
        "                if pair_pos is None:\n",
        "                    break\n",
        "                i = pair_pos\n",
        "                merged = merged[:i] + [merged[i]+merged[i+1]] + merged[i+2:]\n",
        "            pieces.extend(merged)\n",
        "        ids = []\n",
        "        if add_bos: ids.append(self.bos_id)\n",
        "        for p in pieces:\n",
        "            ids.append(self.stoi.get(p, self.pad_id))\n",
        "        if add_eos: ids.append(self.eos_id)\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids: List[int]) -> str:\n",
        "        toks = [self.itos[i] for i in ids if i < len(self.itos)]\n",
        "        toks = [t for t in toks if t not in self.special]\n",
        "        text = ''.join(toks)\n",
        "        return text.replace('‚ñÅ', ' ').strip()\n",
        "\n",
        "# =========================\n",
        "# LoRA utility\n",
        "# =========================\n",
        "class LoRALinear(nn.Module):\n",
        "    def __init__(self, base: nn.Linear, r: int = 8, alpha: float = 16.0, dropout: float = 0.0):\n",
        "        super().__init__()\n",
        "        self.in_features = base.in_features\n",
        "        self.out_features = base.out_features\n",
        "        self.weight = base.weight\n",
        "        self.bias = base.bias\n",
        "        self.r = r\n",
        "        if r > 0:\n",
        "            self.A = nn.Linear(self.in_features, r, bias=False)\n",
        "            self.B = nn.Linear(r, self.out_features, bias=False)\n",
        "            nn.init.kaiming_uniform_(self.A.weight, a=math.sqrt(5))\n",
        "            nn.init.zeros_(self.B.weight)\n",
        "            self.scaling = alpha / r\n",
        "            self.drop = nn.Dropout(dropout)\n",
        "        else:\n",
        "            self.register_parameter('A', None)\n",
        "            self.register_parameter('B', None)\n",
        "            self.drop = nn.Identity()\n",
        "            self.scaling = 1.0\n",
        "    def forward(self, x):\n",
        "        out = F.linear(x, self.weight, self.bias)\n",
        "        if self.r > 0:\n",
        "            lora = self.B(self.A(self.drop(x))) * self.scaling\n",
        "            out = out + lora\n",
        "        return out\n",
        "\n",
        "# =========================\n",
        "# Transformer + Critics\n",
        "# =========================\n",
        "@dataclass\n",
        "class Config:\n",
        "    vocab_size: int\n",
        "    n_layer: int = 6\n",
        "    n_head: int = 8\n",
        "    n_embd: int = 256\n",
        "    block_size: int = 256\n",
        "    lora_r: int = 8\n",
        "    lora_alpha: float = 16.0\n",
        "    dropout: float = 0.1\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, cfg: Config):\n",
        "        super().__init__()\n",
        "        self.n_head = cfg.n_head\n",
        "        self.n_embd = cfg.n_embd\n",
        "        self.key = LoRALinear(nn.Linear(cfg.n_embd, cfg.n_embd), r=cfg.lora_r, alpha=cfg.lora_alpha)\n",
        "        self.query = LoRALinear(nn.Linear(cfg.n_embd, cfg.n_embd), r=cfg.lora_r, alpha=cfg.lora_alpha)\n",
        "        self.value = LoRALinear(nn.Linear(cfg.n_embd, cfg.n_embd), r=cfg.lora_r, alpha=cfg.lora_alpha)\n",
        "        self.proj = LoRALinear(nn.Linear(cfg.n_embd, cfg.n_embd), r=cfg.lora_r, alpha=cfg.lora_alpha)\n",
        "        self.attn_drop = nn.Dropout(cfg.dropout)\n",
        "        self.resid_drop = nn.Dropout(cfg.dropout)\n",
        "        self.register_buffer(\"mask\", torch.tril(torch.ones(cfg.block_size, cfg.block_size)).view(1,1,cfg.block_size,cfg.block_size))\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.size()\n",
        "        assert C % self.n_head == 0, \"n_embd must be divisible by n_head\"\n",
        "        k = self.key(x).view(B,T,self.n_head,C//self.n_head).transpose(1,2)\n",
        "        q = self.query(x).view(B,T,self.n_head,C//self.n_head).transpose(1,2)\n",
        "        v = self.value(x).view(B,T,self.n_head,C//self.n_head).transpose(1,2)\n",
        "        att = (q @ k.transpose(-2,-1)) / math.sqrt(k.size(-1))\n",
        "        att = att.masked_fill(self.mask[:,:,:T,:T]==0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.attn_drop(att)\n",
        "        y = att @ v\n",
        "        y = y.transpose(1,2).contiguous().view(B,T,C)\n",
        "        y = self.resid_drop(self.proj(y))\n",
        "        return y, att\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, cfg: Config):\n",
        "        super().__init__()\n",
        "        self.fc1 = LoRALinear(nn.Linear(cfg.n_embd, 4*cfg.n_embd), r=cfg.lora_r, alpha=cfg.lora_alpha)\n",
        "        self.fc2 = LoRALinear(nn.Linear(4*cfg.n_embd, cfg.n_embd), r=cfg.lora_r, alpha=cfg.lora_alpha)\n",
        "        self.drop = nn.Dropout(cfg.dropout)\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, cfg: Config, with_critic: bool=False):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(cfg.n_embd)\n",
        "        self.attn = CausalSelfAttention(cfg)\n",
        "        self.ln2 = nn.LayerNorm(cfg.n_embd)\n",
        "        self.mlp = MLP(cfg)\n",
        "        self.with_critic = with_critic\n",
        "        if with_critic:\n",
        "            self.critic = nn.Sequential(\n",
        "                nn.Linear(cfg.n_embd, cfg.n_embd//2), nn.GELU(),\n",
        "                nn.Linear(cfg.n_embd//2, 1)\n",
        "            )\n",
        "    def forward(self, x):\n",
        "        y, att = self.attn(self.ln1(x))\n",
        "        x = x + y\n",
        "        y = self.mlp(self.ln2(x))\n",
        "        x = x + y\n",
        "        crit = None\n",
        "        if self.with_critic:\n",
        "            crit = self.critic(x.mean(dim=1).detach())\n",
        "        return x, att, crit\n",
        "\n",
        "class TinyGPT(nn.Module):\n",
        "    def __init__(self, cfg: Config, critic_layers: List[int]):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.tok_emb = nn.Embedding(cfg.vocab_size, cfg.n_embd)\n",
        "        self.pos_emb = nn.Parameter(torch.zeros(1, cfg.block_size, cfg.n_embd))\n",
        "        blocks = []\n",
        "        for i in range(cfg.n_layer):\n",
        "            blocks.append(Block(cfg, with_critic=(i in critic_layers)))\n",
        "        self.blocks = nn.ModuleList(blocks)\n",
        "        self.ln_f = nn.LayerNorm(cfg.n_embd)\n",
        "        self.head = nn.Linear(cfg.n_embd, cfg.vocab_size, bias=False)\n",
        "    def forward(self, idx):\n",
        "        B, T = idx.size(); assert T <= self.cfg.block_size\n",
        "        x = self.tok_emb(idx) + self.pos_emb[:, :T, :]\n",
        "        critic_scores = []\n",
        "        for blk in self.blocks:\n",
        "            x, att, crit = blk(x)\n",
        "            if crit is not None:\n",
        "                critic_scores.append(crit)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "        return logits, critic_scores\n",
        "\n",
        "# =========================\n",
        "# EMA helper\n",
        "# =========================\n",
        "class EMA:\n",
        "    def __init__(self, model: nn.Module, decay: float=0.999):\n",
        "        self.decay = decay\n",
        "        self.shadow = {k: p.clone().detach() for k,p in model.state_dict().items()}\n",
        "    @torch.no_grad()\n",
        "    def update(self, model: nn.Module):\n",
        "        for k, p in model.state_dict().items():\n",
        "            if k in self.shadow:\n",
        "                self.shadow[k].mul_(self.decay).add_(p.detach(), alpha=1-self.decay)\n",
        "    @torch.no_grad()\n",
        "    def copy_to(self, model: nn.Module):\n",
        "        model.load_state_dict(self.shadow, strict=False)\n",
        "\n",
        "# =========================\n",
        "# Replay Buffer\n",
        "# =========================\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, maxlen=4096):\n",
        "        self.maxlen = maxlen\n",
        "        self.data: List[Tuple[str, str, float]] = []\n",
        "    def add(self, prompt: str, response: str, score: float):\n",
        "        self.data.append((prompt, response, score))\n",
        "        if len(self.data) > self.maxlen:\n",
        "            self.data = self.data[-self.maxlen:]\n",
        "    def sample(self, batch_size=16):\n",
        "        return random.sample(self.data, min(batch_size, len(self.data)))\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "# =========================\n",
        "# Utilities\n",
        "# =========================\n",
        "\n",
        "def top_k_logits(logits, k):\n",
        "    v, ix = torch.topk(logits, k)\n",
        "    out = logits.clone()\n",
        "    out[out < v[..., [-1]]] = -float('Inf')\n",
        "    return out\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate(model: TinyGPT, idx: torch.Tensor, max_new_tokens=64, temperature=0.7, top_k_val=20):\n",
        "    model.eval()\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -model.cfg.block_size:]\n",
        "        logits, _ = model(idx_cond)\n",
        "        logits = logits[:, -1, :] / max(1e-6, temperature)\n",
        "        logits = top_k_logits(logits, min(top_k_val, logits.size(-1)))\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        next_id = torch.multinomial(probs, num_samples=1)\n",
        "        idx = torch.cat((idx, next_id), dim=1)\n",
        "        if next_id.item() == 2:  # eos\n",
        "            break\n",
        "    return idx\n",
        "\n",
        "def nll_loss(logits, targets, ignore_index=0):\n",
        "    logits = logits[:, :-1, :].contiguous()\n",
        "    targets = targets[:, 1:].contiguous()\n",
        "    return F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=ignore_index)\n",
        "\n",
        "def kl_divergence(p_logits, q_logits):\n",
        "    p_logprob = F.log_softmax(p_logits, dim=-1)\n",
        "    q_prob = F.softmax(q_logits, dim=-1)\n",
        "    return F.kl_div(p_logprob, q_prob, reduction='batchmean')\n",
        "\n",
        "# =========================\n",
        "# Training / Sleep Loops\n",
        "# =========================\n",
        "@dataclass\n",
        "class TrainCfg:\n",
        "    block_size: int = 256\n",
        "    critic_layers: tuple = (1, 3, 5)\n",
        "    lr_D: float = 5e-4\n",
        "    lr_G: float = 1e-4\n",
        "    ttur_D_steps: int = 3\n",
        "    ttur_G_steps: int = 1\n",
        "    ema_decay: float = 0.999\n",
        "    device: str = 'cpu'\n",
        "\n",
        "class EGTSystem:\n",
        "    def __init__(self, tokenizer: TinyBPETokenizer, cfg: TrainCfg):\n",
        "        self.tok = tokenizer\n",
        "        model_cfg = Config(vocab_size=tokenizer.vocab_size, block_size=cfg.block_size)\n",
        "        self.G = TinyGPT(model_cfg, critic_layers=list(cfg.critic_layers)).to(cfg.device)\n",
        "        self.Teacher = TinyGPT(model_cfg, critic_layers=[]).to(cfg.device)\n",
        "        self.Teacher.load_state_dict(self.G.state_dict(), strict=False)\n",
        "        self.ema = EMA(self.Teacher, decay=cfg.ema_decay)\n",
        "        D_params = []\n",
        "        for blk in self.G.blocks:\n",
        "            if getattr(blk, 'with_critic', False):\n",
        "                D_params += list(blk.critic.parameters())\n",
        "        self.D_params = D_params\n",
        "        for name, p in self.G.named_parameters():\n",
        "            if isinstance(p, torch.nn.Parameter):\n",
        "                p.requires_grad = (('A.weight' in name) or ('B.weight' in name) or ('critic' in name))\n",
        "        self.optD = torch.optim.AdamW(self.D_params, lr=cfg.lr_D, betas=(0.5, 0.999))\n",
        "        g_params = [p for n,p in self.G.named_parameters() if p.requires_grad and ('critic' not in n)]\n",
        "        self.G_params = g_params\n",
        "        self.optG = torch.optim.AdamW(self.G_params, lr=cfg.lr_G)\n",
        "        self.cfg = cfg\n",
        "        self.buffer = ReplayBuffer(maxlen=4096)\n",
        "\n",
        "    def encode_batch(self, pairs: List[Tuple[str,str,float]], pad_to=None):\n",
        "        prompts = [p for p,_,_ in pairs]\n",
        "        resps = [r for _,r,_ in pairs]\n",
        "        xs = [self.tok.encode(p, add_bos=True, add_eos=False) for p in prompts]\n",
        "        ys = [self.tok.encode(r, add_bos=False, add_eos=True) for r in resps]\n",
        "        seqs = [x+y for x,y in zip(xs,ys)]\n",
        "        maxlen = pad_to or max(len(s) for s in seqs)\n",
        "        out = torch.full((len(seqs), maxlen), self.tok.pad_id, dtype=torch.long)\n",
        "        for i,s in enumerate(seqs):\n",
        "            s = s[:maxlen]\n",
        "            out[i,:len(s)] = torch.tensor(s)\n",
        "        return out.to(self.cfg.device)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def wake_step(self, prompt: str, temperature=0.7, top_k_val=20):\n",
        "        self.G.eval()\n",
        "        idx = torch.tensor([self.tok.encode(prompt)], dtype=torch.long, device=self.cfg.device)\n",
        "        out = generate(self.G, idx, max_new_tokens=64, temperature=temperature, top_k_val=top_k_val)\n",
        "        full_ids = out[0].tolist()\n",
        "        # continuation only\n",
        "        prompt_len = len(self.tok.encode(prompt))\n",
        "        gen_ids = full_ids[prompt_len:]\n",
        "        text = self.tok.decode(gen_ids)\n",
        "        tokens = text.split()\n",
        "        if len(tokens) < 2:\n",
        "            score = 0.0\n",
        "        else:\n",
        "            bigrams = list(zip(tokens, tokens[1:]))\n",
        "            score = len(set(bigrams)) / max(1, len(bigrams))\n",
        "        self.buffer.add(prompt, text, score)\n",
        "        return text, score\n",
        "\n",
        "    def sleep_epoch(self, steps_D=30, steps_G=10, batch_size=4, fm_lambda=5.0, beta_gan=0.3, alpha_kld=0.7):\n",
        "        if len(self.buffer) < batch_size:\n",
        "            return {'lossD':0.0, 'lossG':0.0}\n",
        "        self.G.train(); self.Teacher.eval()\n",
        "        bce = nn.BCEWithLogitsLoss()\n",
        "        lossD_avg = 0.0; lossG_avg = 0.0\n",
        "        # D steps\n",
        "        for _ in range(steps_D):\n",
        "            batch = self.buffer.sample(batch_size)\n",
        "            real_idx = self.encode_batch(batch)\n",
        "            _, real_scores = self.G(real_idx)\n",
        "            prompts = [p for p,_,_ in batch]\n",
        "            fake_pairs = []\n",
        "            for p in prompts:\n",
        "                txt,_ = self.wake_step(p)\n",
        "                fake_pairs.append((p, txt, 0.0))\n",
        "            fake_idx = self.encode_batch(fake_pairs)\n",
        "            _, fake_scores = self.G(fake_idx)\n",
        "            self.optD.zero_grad()\n",
        "            lossD = 0.0\n",
        "            for rs, fs in zip(real_scores, fake_scores):\n",
        "                y_real = torch.ones_like(rs)\n",
        "                y_fake = torch.zeros_like(fs)\n",
        "                lossD = lossD + bce(rs, y_real) + bce(fs, y_fake)\n",
        "            lossD.backward(); nn.utils.clip_grad_norm_(self.D_params, 1.0); self.optD.step()\n",
        "            lossD_avg += float(lossD.item())\n",
        "        # G steps\n",
        "        for _ in range(steps_G):\n",
        "            batch = self.buffer.sample(batch_size)\n",
        "            idx = self.encode_batch(batch)\n",
        "            with torch.no_grad():\n",
        "                t_logits, _ = self.Teacher(idx)\n",
        "            s_logits, s_scores = self.G(idx)\n",
        "            loss_kld = kl_divergence(s_logits, t_logits)\n",
        "            prompts = [p for p,_,_ in batch]\n",
        "            fake_pairs = []\n",
        "            for p in prompts:\n",
        "                txt,_ = self.wake_step(p)\n",
        "                fake_pairs.append((p, txt, 0.0))\n",
        "            fake_idx = self.encode_batch(fake_pairs)\n",
        "            _, fake_scores = self.G(fake_idx)\n",
        "            gan_loss = 0.0; fm_loss = 0.0\n",
        "            for fs in fake_scores:\n",
        "                y_real = torch.ones_like(fs)\n",
        "                gan_loss = gan_loss + bce(fs, y_real)\n",
        "            if s_scores and fake_scores:\n",
        "                s_cat = torch.cat(s_scores, dim=1)\n",
        "                f_cat = torch.cat(fake_scores, dim=1)\n",
        "                fm_loss = F.mse_loss(s_cat.mean(dim=0), f_cat.mean(dim=0))\n",
        "            loss = (alpha_kld*loss_kld) + (beta_gan*(gan_loss + fm_lambda*fm_loss))\n",
        "            self.optG.zero_grad(); loss.backward(); nn.utils.clip_grad_norm_(self.G_params, 1.0); self.optG.step()\n",
        "            lossG_avg += float(loss.item())\n",
        "            self.ema.update(self.G); self.ema.copy_to(self.Teacher)\n",
        "        return {'lossD': lossD_avg/max(1,steps_D), 'lossG': lossG_avg/max(1,steps_G)}\n",
        "\n",
        "# =========================\n",
        "# Demo / Main\n",
        "# =========================\n",
        "TOY_TEXT = [\n",
        "    \"Once upon a time, there was a curious mind.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"Mathematics is the music of reason.\",\n",
        "    \"In the beginning, there was only the word.\",\n",
        "    \"We are such stuff as dreams are made on.\",\n",
        "]\n",
        "EXTRA_CORPUS = [\n",
        "    \"Curiosity leads us down paths we do not expect.\",\n",
        "    \"Numbers and patterns reveal hidden structure.\",\n",
        "    \"Stories begin, worlds unfold, characters change.\",\n",
        "    \"Simple explanations often hide deep ideas.\",\n",
        "]\n",
        "PROMPTS = [\n",
        "    \"Tell me a fable about curiosity:\",\n",
        "    \"Write a sentence using every letter:\",\n",
        "    \"Explain prime numbers simply:\",\n",
        "    \"Begin a creation myth:\",\n",
        "    \"Quote Shakespeare and continue:\",\n",
        "]\n",
        "\n",
        "def main():\n",
        "    random.seed(0); torch.manual_seed(0)\n",
        "    tok = TinyBPETokenizer(TOY_TEXT + EXTRA_CORPUS + PROMPTS, vocab_size=300)\n",
        "    cfg = TrainCfg(device='cpu')\n",
        "    sys = EGTSystem(tok, cfg)\n",
        "    print(\"[wake] seeding replay buffer...\")\n",
        "    for _ in range(24):\n",
        "        p = random.choice(PROMPTS)\n",
        "        txt, sc = sys.wake_step(p)\n",
        "        print(f\"  prompt: {p}\\n  resp: {txt}\\n  score:{sc:.3f}\")\n",
        "    for epoch in range(3):\n",
        "        print(f\"\\n[sleep] epoch {epoch}\")\n",
        "        stats = sys.sleep_epoch(steps_D=30, steps_G=10, batch_size=4)\n",
        "        print(\"  lossD=%.4f lossG=%.4f\" % (stats['lossD'], stats['lossG']))\n",
        "        p = random.choice(PROMPTS)\n",
        "        out, sc = sys.wake_step(p)\n",
        "        print(f\"[wake] prompt: {p}\\n  resp: {out}\\n  score:{sc:.3f}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BLWkE4gUW0n",
        "outputId": "e520ea9c-0b9e-4804-e6c5-c9d5f1501bf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[wake] seeding replay buffer...\n",
            "  prompt: Begin a creation myth:\n",
            "  resp: not q jumpati u le time, quicsver therel brb hi brownlyte beginning, made mBer reason. beginning,on word.p Sgon expect. exp reas Mathematic\n",
            "  score:1.000\n",
            "  prompt: Begin a creation myth:\n",
            "  resp: quick. anhao br p pattern qu reveal patter hidden Oncewn beginning,nceation doncumberseg  qu brownimp le Mathe on stu stuf ratiimp a rehaha and Curiosityte. ti stufy su us thesit beginning,a dreaers and up expect.des us stuing expe: lazy\n",
            "  score:1.000\n",
            "  prompt: Tell me a fable about curiosity:\n",
            "  resp: la revea revea wor C path le of over su s lazy dreamsing beginningati u hi not or i suc Matteruri p hi hiders hi upon ut on lazyers wor on stu reason. qu patterns uu lea rev curious st Mathematictter of expe pa upon ne muB on. C is titt\n",
            "  score:1.000\n",
            "  prompt: Explain prime numbers simply:\n",
            "  resp: nce Mathematic Th stuff us sucd begin anurio up o only j Numberseati Mathemttlation no leads reas q jumply b To Od. stu j: us asndslati Mathematicsd begin anha o qui q timect suchha downwn  Mathe Mathematisi min lad.\n",
            "  score:1.000\n",
            "  prompt: Quote Shakespeare and continue:\n",
            "  resp: oum\n",
            "  score:0.000\n",
            "  prompt: Begin a creation myth:\n",
            "  resp: Q word. we musiing madeationttnce ti are lazy.ers Mathematictter ot over expect.de uponre quimp jumps dream n.u lea s br dog. are hi upon hiddenn mu upon c We hidde:unceimp was Mathematicin b on not suynce b time patterri\n",
            "  score:1.000\n",
            "  prompt: Begin a creation myth:\n",
            "  resp: patterncr chawn such notatio made Mathematic reas M M of rers hif expect quickawn MEB was hiddenthwn tieronim up stuff us suc are hict Mathematiod.atio: a an Th expect b on musicon.es b T uslyteers\n",
            "  score:1.000\n",
            "  prompt: Explain prime numbers simply:\n",
            "  resp: patterri Cawn such exp dog. time curiou O hiatia paths lazy tier on. reveal Numbers o pasit path jumpsnc su Once f M: us there,o O suc W wever f Th music stu revea expeas revea: not mu In patterns on. of suc only fox dre In\n",
            "  score:1.000\n",
            "  prompt: Begin a creation myth:\n",
            "  resp: notn quic tiea an:u made expe jumpssi Os reveal noct on.umtinc M we musiing Ma worg patternsing bas I down e expe jumpsim beginning, us stu revea beginn Mathematiceg uponQ are shao curious hidsly fo suc exp dog. ptter:\n",
            "  score:1.000\n",
            "  prompt: Explain prime numbers simply:\n",
            "  resp: wor: musierssit\n",
            "  score:1.000\n",
            "  prompt: Quote Shakespeare and continue:\n",
            "  resp: Curiosityon Ttinc reason. qu brown df jumre Curionurio b n m brown patterB pattero Numbers b not paths lazyegon expect.th mB on. of expe cd c Curio expect. jumps imp wasea andte quic C over suonure us the theth m reveal was\n",
            "  score:1.000\n",
            "  prompt: Write a sentence using every letter:\n",
            "  resp: patter bdsuBer on. expect. musicly b dreams  Mathekncb\n",
            "  score:1.000\n",
            "  prompt: Quote Shakespeare and continue:\n",
            "  resp: ttkurio andla C path isuri muwn are s lazyb mu expe pa Ma dog.k Mathematiceg on.l br b Mathemati woct tier a brb ande are jumpsim pattern we In leads O curious ti beginning,onegin suc only pa fo suc in w pattern Od. us us\n",
            "  score:1.000\n",
            "  prompt: Write a sentence using every letter:\n",
            "  resp: ationri beginning overuri quicer S revla musiinea S suy su jumps la patternver not quic O Mathematics Od.ds h br u su drea quickes br hiumnati brownimp suc  lea time we sucum revea revea madeon. pE dre hid jumpsnce suc onlyuti\n",
            "  score:1.000\n",
            "  prompt: Explain prime numbers simply:\n",
            "  resp: laz qtt su jumps do h br beginning,a and We u eing expeationde doumge lazy dreams beginning,uri quic dreams u such C onlydurio theds the muea Sing su such not us the sucher over wo I We br ut beginn Mathematic\n",
            "  score:1.000\n",
            "  prompt: Write a sentence using every letter:\n",
            "  resp: patter min ti bro q stu mBim uponBumbers wor dre lea an Oim upon pasit not su st upon hidden beginning,uriddsule reveallyu downnumb S ma l bver there Mathematic Mathem st hiddenn beginning Once Son.es revea d We Numbers andri reason.\n",
            "  score:1.000\n",
            "  prompt: Tell me a fable about curiosity:\n",
            "  resp: Q mintter reason. qu made jumps ma Sctncre, curiousti drela chala cha s Curiosity patter j down mind. st Mathematictterha e curiou Od. expe cha sea st burio mind.Q ti stuf m drea I down an br hi weveron beginning musication curio WeEteha\n",
            "  score:1.000\n",
            "  prompt: Quote Shakespeare and continue:\n",
            "  resp: time,r Mathere qu made mBwnB leadswn:sQ chaQ mine mu curioo mB leads and hiddely T d lazy j Mathematic reaser only fox areon. reason. leads hum expethe wor dreuri,d Numbers re overti lazy. an patter hidden lea c oct ti\n",
            "  score:1.000\n",
            "  prompt: Explain prime numbers simply:\n",
            "  resp: lazegti minver o mind. hidrias reveaaim suc W beginning notn music asncr a e We br hie upono curiosit up us bn muwnEre O ti beginri do leads Ouri pum us we m la ares revea time, drelact jumps\n",
            "  score:1.000\n",
            "  prompt: Quote Shakespeare and continue:\n",
            "  resp: patter hieg wort expeation doe Mathete beginning, is sucum chayuBin bon expeas reveal patterns quicsit not. is of Mathematic cha pattern on.si min made hid jumps word. hiE reason. qu madeationdete and hi brown Mathemati wo Weegin  jum time do mind. patter\n",
            "  score:1.000\n",
            "  prompt: Quote Shakespeare and continue:\n",
            "  resp: b reveal no up b s usa wou expe patter hiddenuriosityon no qtt in We qui q hidden and over in wtver curiouinB leads beginning, hi In exp reas expect word.o made tierct Mathematic\n",
            "  score:1.000\n",
            "  prompt: Write a sentence using every letter:\n",
            "  resp: ation uss mind.: Oncewn ti begin was titt snce p quic quick curio la Mathematic T.urioreuri on. dog reas Mathematic T. a  qu Thelyte st Curiosity us we In brhao music madeation expe par su drea revea revea dth lazy stuff reveal oB dream\n",
            "  score:1.000\n",
            "  prompt: Explain prime numbers simply:\n",
            "  resp: laz jump brown Mathemati curious hidersks reveal h u drea musi made tiing bon expect. jumps word. dreams downwn: there pwn: there leadsthe stuffBti Mathematics hidde laz we hieon. p curious su curioo: S brownin expen Curiosity jum Mathematic Numbers andri i asQ\n",
            "  score:1.000\n",
            "  prompt: Tell me a fable about curiosity:\n",
            "  resp: tt quick onwn path expect. exp over not su us patter Mathematics mun Curiosity Th stu wor C le Curiosity jum N over only mu curio the expect leaingth n O anddect on. Mathemati beginn wor as Thesity m st Curiosity wep stu m eati s ma lazy dreams beginning, u j made\n",
            "  score:1.000\n",
            "\n",
            "[sleep] epoch 0\n",
            "  lossD=4.1689 lossG=0.6770\n",
            "[wake] prompt: Quote Shakespeare and continue:\n",
            "  resp: laz b revealon.tiones ThQ word.atiimp le jump: musiinthplever on.l wo ti begin was time,ati Mathematic T hidde hiatio: usme In timewn beginning quica Mathe b not usme MatheEme on stunce Mathematic\n",
            "  score:1.000\n",
            "\n",
            "[sleep] epoch 1\n",
            "  lossD=4.1779 lossG=0.6265\n",
            "[wake] prompt: Explain prime numbers simply:\n",
            "  resp: rio sucer only such pk h brnceimp foa reveag patterns on. Mathemati I expeationttla: Once us sucing learersing hid quicktheri b T hidde lazy Onceim not quic expect onas mu j down mind.ri b s worguri,the stuff b\n",
            "  score:1.000\n",
            "\n",
            "[sleep] epoch 2\n",
            "  lossD=4.1872 lossG=0.6398\n",
            "[wake] prompt: Write a sentence using every letter:\n",
            "  resp: ationri curio fox pum lazy j paths we la muwn leads h on. Mathemati In hi st musi min curio laz we T su N reas expends quicka and Weing M overtion word. fox areg suc Maea on stu reveaa df no leadsef Mathemati qui overti\n",
            "  score:1.000\n"
          ]
        }
      ]
    }
  ]
}
